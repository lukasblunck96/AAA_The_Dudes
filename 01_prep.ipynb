{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from shapely.geometry import Point, MultiPolygon\n",
    "import vaex\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pyarrow as pa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_taxi_trips_all = vaex.open('./data/trips.hdf5')\n",
    "\n",
    "### Replace spaces and uppercases in column names\n",
    "column_names = df_taxi_trips_all.column_names\n",
    "column_names_refactored = [ln.replace(' ', '_').lower() for ln in column_names]\n",
    "\n",
    "for i, column in enumerate(column_names):\n",
    "    df_taxi_trips_all.rename(column, column_names_refactored[i])\n",
    "\n",
    "# correct typo\n",
    "df_taxi_trips_all.rename(\"dropoff_centroid__location\",\"dropoff_centroid_location\")\n",
    "\n",
    "# cast timestamp columns to datetime\n",
    "date_format = \"%m/%d/%Y %I:%M:%S %p\"\n",
    "def column_to_datetime(datetime_str):\n",
    "    return np.datetime64(datetime.strptime(datetime_str, date_format))\n",
    "\n",
    "df_taxi_trips_all['trip_start_timestamp'] = df_taxi_trips_all['trip_start_timestamp'].apply(column_to_datetime)\n",
    "df_taxi_trips_all['trip_end_timestamp'] = df_taxi_trips_all['trip_end_timestamp'].apply(column_to_datetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open external data: census tracts\n",
    "df_census_tracts = vaex.open('./data/chicago_census_tracts.csv')\n",
    "df_census_tracts.rename(\"the_geom\", \"geometry\")\n",
    "\n",
    "# community areas\n",
    "df_community_areas = vaex.open('./data/community_areas.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total amount of trips: 24,988,003\n"
     ]
    }
   ],
   "source": [
    "total_trips = len(df_taxi_trips_all)\n",
    "print(f\"Total amount of trips: {total_trips:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trips with trip_miles = 0: 3,003,697\n",
      "Number of trips without trip miles and same location: 1,387,241\n"
     ]
    }
   ],
   "source": [
    "# Number of trips with trip_miles = 0\n",
    "zero_trip_miles = len(df_taxi_trips_all[df_taxi_trips_all[\"trip_miles\"] == 0])\n",
    "print(f\"Number of trips with trip_miles = 0: {zero_trip_miles:,}\")\n",
    "\n",
    "# Number of trips with trip_miles = 0 and no difference in pickup and dropoff location\n",
    "zero_trip_miles_same_loc = len(df_taxi_trips_all[(df_taxi_trips_all[\"trip_miles\"] == 0) & (df_taxi_trips_all[\"pickup_centroid_location\"] == df_taxi_trips_all[\"dropoff_centroid_location\"])])\n",
    "print(f\"Number of trips without trip miles and same location: {zero_trip_miles_same_loc:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Trips with Zero Trip Miles and Same Pickup/Dropoff Locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Trips with Non-Zero Trip Miles and Different Pickup/Dropoff Locations: 23,600,762\n"
     ]
    }
   ],
   "source": [
    "# drop rows without trip miles and same location\n",
    "df_non_zero_trip_miles = df_taxi_trips_all[(df_taxi_trips_all[\"trip_miles\"] != 0) | (df_taxi_trips_all[\"pickup_centroid_location\"] != df_taxi_trips_all[\"dropoff_centroid_location\"])]\n",
    "print(f\"Total Trips with Non-Zero Trip Miles and Different Pickup/Dropoff Locations: {len(df_non_zero_trip_miles):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insert Missing Census Tract IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of missing pickup_census_tract IDs: 30.76%\n",
      "Percentage of missing pickup_census_tract IDs: 31.13%\n"
     ]
    }
   ],
   "source": [
    "# Calculate the percentage of missing pickup_census_tract IDs\n",
    "percentage_missing_pickup_census_tract = (len(df_non_zero_trip_miles[df_non_zero_trip_miles[\"pickup_census_tract\"].isnan()]) / len(df_non_zero_trip_miles)) * 100\n",
    "percentage_missing_dropoff_census_tract = (len(df_non_zero_trip_miles[df_non_zero_trip_miles[\"dropoff_census_tract\"].isnan()]) / len(df_non_zero_trip_miles)) * 100\n",
    "\n",
    "# Round the percentage to two decimal places\n",
    "percentage_rounded_pickup = round(percentage_missing_pickup_census_tract, 2)\n",
    "percentage_rounded_dropoff = round(percentage_missing_dropoff_census_tract, 2)\n",
    "\n",
    "# Display the result\n",
    "print(f\"Percentage of missing pickup_census_tract IDs: {percentage_rounded_pickup}%\")\n",
    "print(f\"Percentage of missing pickup_census_tract IDs: {percentage_rounded_dropoff}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Trips without Rows with Null Values in \"X_centroid_location\" AND \"X_census_tract\": 21,170,643\n"
     ]
    }
   ],
   "source": [
    "# Drop all rows where both \"X_census_tract\" and \"X_centroid_location\" are null\n",
    "# We keep rows WITH \"X_centroid_location\" and WITHOUT \"pickup_census_tract\" to craft census tracts\n",
    "df_cleaned_census_and_location = df_non_zero_trip_miles.dropna(column_names=[\"pickup_census_tract\", \"pickup_centroid_location\"], how=\"all\")\n",
    "df_cleaned_census_and_location = df_cleaned_census_and_location.dropna(column_names=[\"dropoff_census_tract\", \"dropoff_centroid_location\"], how=\"all\")\n",
    "print(f\"Total Trips without Rows with Null Values in \\\"X_centroid_location\\\" AND \\\"X_census_tract\\\": {len(df_cleaned_census_and_location):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Rows where \"X_census_tract\" is Null: 5,000,517\n"
     ]
    }
   ],
   "source": [
    "df_no_census_tract_both = df_cleaned_census_and_location[df_cleaned_census_and_location[\"pickup_census_tract\"].isnan() | df_cleaned_census_and_location[\"dropoff_census_tract\"].isnan()]\n",
    "print(f\"Number of Rows where \\\"X_census_tract\\\" is Null: {len(df_no_census_tract_both):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/lblunck/DEVELOPMENT/AAA_The_Dudes/01_prep.ipynb Cell 13\u001b[0m in \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/lblunck/DEVELOPMENT/AAA_The_Dudes/01_prep.ipynb#Y202sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Assuming you have one dataframe containing null values for both pickup_census_tract and dropoff_census_tract\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/lblunck/DEVELOPMENT/AAA_The_Dudes/01_prep.ipynb#Y202sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m df_no_census_tract_both \u001b[39m=\u001b[39m df_no_census_tract_both\u001b[39m.\u001b[39;49mto_pandas_df()\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/lblunck/DEVELOPMENT/AAA_The_Dudes/01_prep.ipynb#Y202sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# Convert the pickup_centroid_location in the dataframe to Point geometries\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/lblunck/DEVELOPMENT/AAA_The_Dudes/01_prep.ipynb#Y202sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m df_no_census_tract_both[\u001b[39m'\u001b[39m\u001b[39mpickup_centroid_location\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df_no_census_tract_both\u001b[39m.\u001b[39mapply(\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/lblunck/DEVELOPMENT/AAA_The_Dudes/01_prep.ipynb#Y202sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39mlambda\u001b[39;00m row: Point(row[\u001b[39m'\u001b[39m\u001b[39mpickup_centroid_longitude\u001b[39m\u001b[39m'\u001b[39m], row[\u001b[39m'\u001b[39m\u001b[39mpickup_centroid_latitude\u001b[39m\u001b[39m'\u001b[39m]), axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/lblunck/DEVELOPMENT/AAA_The_Dudes/01_prep.ipynb#Y202sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/vaex/dataframe.py:3341\u001b[0m, in \u001b[0;36mDataFrame.to_pandas_df\u001b[0;34m(self, column_names, selection, strings, virtual, index_name, parallel, chunk_size, array_type)\u001b[0m\n\u001b[1;32m   3339\u001b[0m     \u001b[39mreturn\u001b[39;00m iterator()\n\u001b[1;32m   3340\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 3341\u001b[0m     \u001b[39mreturn\u001b[39;00m create_pdf(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mto_dict(column_names\u001b[39m=\u001b[39;49mcolumn_names, selection\u001b[39m=\u001b[39;49mselection, parallel\u001b[39m=\u001b[39;49mparallel, array_type\u001b[39m=\u001b[39;49marray_type))\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/vaex/dataframe.py:3256\u001b[0m, in \u001b[0;36mDataFrame.to_dict\u001b[0;34m(self, column_names, selection, strings, virtual, parallel, chunk_size, array_type)\u001b[0m\n\u001b[1;32m   3254\u001b[0m             \u001b[39myield\u001b[39;00m i1, i2, \u001b[39mdict\u001b[39m(\u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(column_names, [array_types\u001b[39m.\u001b[39mconvert(chunk, array_type) \u001b[39mfor\u001b[39;00m chunk \u001b[39min\u001b[39;00m chunks])))\n\u001b[1;32m   3255\u001b[0m     \u001b[39mreturn\u001b[39;00m iterator()\n\u001b[0;32m-> 3256\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mdict\u001b[39m(\u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(column_names, [array_types\u001b[39m.\u001b[39mconvert(chunk, array_type) \u001b[39mfor\u001b[39;00m chunk \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mevaluate(column_names, selection\u001b[39m=\u001b[39;49mselection, parallel\u001b[39m=\u001b[39;49mparallel)])))\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/vaex/dataframe.py:3095\u001b[0m, in \u001b[0;36mDataFrame.evaluate\u001b[0;34m(self, expression, i1, i2, out, selection, filtered, array_type, parallel, chunk_size, progress)\u001b[0m\n\u001b[1;32m   3093\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mevaluate_iterator(expression, s1\u001b[39m=\u001b[39mi1, s2\u001b[39m=\u001b[39mi2, out\u001b[39m=\u001b[39mout, selection\u001b[39m=\u001b[39mselection, filtered\u001b[39m=\u001b[39mfiltered, array_type\u001b[39m=\u001b[39marray_type, parallel\u001b[39m=\u001b[39mparallel, chunk_size\u001b[39m=\u001b[39mchunk_size, progress\u001b[39m=\u001b[39mprogress)\n\u001b[1;32m   3094\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 3095\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_evaluate_implementation(expression, i1\u001b[39m=\u001b[39;49mi1, i2\u001b[39m=\u001b[39;49mi2, out\u001b[39m=\u001b[39;49mout, selection\u001b[39m=\u001b[39;49mselection, filtered\u001b[39m=\u001b[39;49mfiltered, array_type\u001b[39m=\u001b[39;49marray_type, parallel\u001b[39m=\u001b[39;49mparallel, chunk_size\u001b[39m=\u001b[39;49mchunk_size, progress\u001b[39m=\u001b[39;49mprogress)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/vaex/dataframe.py:6494\u001b[0m, in \u001b[0;36mDataFrameLocal._evaluate_implementation\u001b[0;34m(self, expression, i1, i2, out, selection, filtered, array_type, parallel, chunk_size, raw, progress)\u001b[0m\n\u001b[1;32m   6492\u001b[0m             arrays[expression][i1:i2] \u001b[39m=\u001b[39m blocks[i]\n\u001b[1;32m   6493\u001b[0m \u001b[39mif\u001b[39;00m expression_to_evaluate:\n\u001b[0;32m-> 6494\u001b[0m     df\u001b[39m.\u001b[39;49mmap_reduce(assign, \u001b[39mlambda\u001b[39;49;00m \u001b[39m*\u001b[39;49m_: \u001b[39mNone\u001b[39;49;00m, expression_to_evaluate, progress\u001b[39m=\u001b[39;49mprogress, ignore_filter\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, selection\u001b[39m=\u001b[39;49mselection, pre_filter\u001b[39m=\u001b[39;49muse_filter, info\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, to_numpy\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mevaluate\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m   6495\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfinalize_result\u001b[39m(expression):\n\u001b[1;32m   6496\u001b[0m     expression_obj \u001b[39m=\u001b[39m expression\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/vaex/dataframe.py:438\u001b[0m, in \u001b[0;36mDataFrame.map_reduce\u001b[0;34m(self, map, reduce, arguments, progress, delay, info, to_numpy, ignore_filter, pre_filter, name, selection)\u001b[0m\n\u001b[1;32m    436\u001b[0m progressbar\u001b[39m.\u001b[39madd_task(task, \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmap reduce: \u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m    437\u001b[0m task \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexecutor\u001b[39m.\u001b[39mschedule(task)\n\u001b[0;32m--> 438\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_delay(delay, task)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/vaex/dataframe.py:1780\u001b[0m, in \u001b[0;36mDataFrame._delay\u001b[0;34m(self, delay, task, progressbar)\u001b[0m\n\u001b[1;32m   1778\u001b[0m     \u001b[39mreturn\u001b[39;00m task\n\u001b[1;32m   1779\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1780\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexecute()\n\u001b[1;32m   1781\u001b[0m     \u001b[39mreturn\u001b[39;00m task\u001b[39m.\u001b[39mget()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/vaex/dataframe.py:421\u001b[0m, in \u001b[0;36mDataFrame.execute\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    419\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mrepr\u001b[39m(task))\n\u001b[1;32m    420\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexecutor\u001b[39m.\u001b[39mtasks:\n\u001b[0;32m--> 421\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexecutor\u001b[39m.\u001b[39;49mexecute()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/vaex/execution.py:308\u001b[0m, in \u001b[0;36mExecutorLocal.execute\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mexecute\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 308\u001b[0m     \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexecute_generator():\n\u001b[1;32m    309\u001b[0m         \u001b[39mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/vaex/execution.py:432\u001b[0m, in \u001b[0;36mExecutorLocal.execute_generator\u001b[0;34m(self, use_async)\u001b[0m\n\u001b[1;32m    430\u001b[0m         logger\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mPass cancelled because of the global progress event: \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignal_progress\u001b[39m.\u001b[39mcallbacks)\n\u001b[1;32m    431\u001b[0m     \u001b[39mreturn\u001b[39;00m ok_tasks \u001b[39mand\u001b[39;00m ok_executor \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m all_stopped\n\u001b[0;32m--> 432\u001b[0m \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mthread_pool\u001b[39m.\u001b[39mmap(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocess_part, dataset\u001b[39m.\u001b[39mchunk_iterator(run\u001b[39m.\u001b[39mdataset_deps, chunk_size),\n\u001b[1;32m    433\u001b[0m                                     dataset\u001b[39m.\u001b[39mrow_count,\n\u001b[1;32m    434\u001b[0m                                     progress\u001b[39m=\u001b[39mprogress,\n\u001b[1;32m    435\u001b[0m                                     cancel\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cancel(run), unpack\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, run\u001b[39m=\u001b[39mrun, use_async\u001b[39m=\u001b[39muse_async)\n\u001b[1;32m    436\u001b[0m duration_wallclock \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m t0\n\u001b[1;32m    437\u001b[0m logger\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mexecuting took \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m seconds\u001b[39m\u001b[39m\"\u001b[39m, duration_wallclock)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/vaex/multithreading.py:104\u001b[0m, in \u001b[0;36mThreadPoolIndex.map\u001b[0;34m(self, callable, iterator, count, on_error, progress, cancel, unpack, use_async, **kwargs_extra)\u001b[0m\n\u001b[1;32m    102\u001b[0m iterator \u001b[39m=\u001b[39m \u001b[39miter\u001b[39m(buffer(iterator, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_max_workers \u001b[39m+\u001b[39m \u001b[39m3\u001b[39m))\n\u001b[1;32m    103\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 104\u001b[0m     \u001b[39mfor\u001b[39;00m value \u001b[39min\u001b[39;00m iterator:\n\u001b[1;32m    105\u001b[0m         \u001b[39mif\u001b[39;00m use_async:\n\u001b[1;32m    106\u001b[0m             value \u001b[39m=\u001b[39m \u001b[39myield\u001b[39;00m value\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/vaex/itertools.py:5\u001b[0m, in \u001b[0;36mbuffer\u001b[0;34m(i, n)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m      4\u001b[0m     \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[0;32m----> 5\u001b[0m         values\u001b[39m.\u001b[39mappend(\u001b[39mnext\u001b[39;49m(i))\n\u001b[1;32m      6\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m      7\u001b[0m         values\u001b[39m.\u001b[39mappend(\u001b[39mnext\u001b[39m(i))\n",
      "File \u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py:621\u001b[0m, in \u001b[0;36mExecutor.map.<locals>.result_iterator\u001b[0;34m()\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[39mwhile\u001b[39;00m fs:\n\u001b[1;32m    619\u001b[0m     \u001b[39m# Careful not to keep a reference to the popped future\u001b[39;00m\n\u001b[1;32m    620\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 621\u001b[0m         \u001b[39myield\u001b[39;00m _result_or_cancel(fs\u001b[39m.\u001b[39;49mpop())\n\u001b[1;32m    622\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    623\u001b[0m         \u001b[39myield\u001b[39;00m _result_or_cancel(fs\u001b[39m.\u001b[39mpop(), end_time \u001b[39m-\u001b[39m time\u001b[39m.\u001b[39mmonotonic())\n",
      "File \u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py:319\u001b[0m, in \u001b[0;36m_result_or_cancel\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    318\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 319\u001b[0m         \u001b[39mreturn\u001b[39;00m fut\u001b[39m.\u001b[39;49mresult(timeout)\n\u001b[1;32m    320\u001b[0m     \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    321\u001b[0m         fut\u001b[39m.\u001b[39mcancel()\n",
      "File \u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py:453\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39m==\u001b[39m FINISHED:\n\u001b[1;32m    451\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__get_result()\n\u001b[0;32m--> 453\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_condition\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[1;32m    456\u001b[0m     \u001b[39mraise\u001b[39;00m CancelledError()\n",
      "File \u001b[0;32m/usr/lib/python3.10/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[39mtry\u001b[39;00m:    \u001b[39m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         waiter\u001b[39m.\u001b[39;49macquire()\n\u001b[1;32m    321\u001b[0m         gotit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Assuming you have one dataframe containing null values for both pickup_census_tract and dropoff_census_tract\n",
    "df_no_census_tract_both = df_no_census_tract_both.to_pandas_df()\n",
    "\n",
    "# Convert the pickup_centroid_location in the dataframe to Point geometries\n",
    "df_no_census_tract_both['pickup_centroid_location'] = df_no_census_tract_both.apply(\n",
    "    lambda row: Point(row['pickup_centroid_longitude'], row['pickup_centroid_latitude']), axis=1\n",
    ")\n",
    "\n",
    "# Prepare a function to find the census tract for a given point\n",
    "def find_census_tract(point, census_tract_df):\n",
    "    for index, row in census_tract_df.iterrows():\n",
    "        if point.within(row['geometry']):\n",
    "            return row['GEOID10']\n",
    "    return None\n",
    "\n",
    "# Create dictionaries to store the census tract IDs for pickup and dropoff points\n",
    "pickup_census_tract_ids = {}\n",
    "dropoff_census_tract_ids = {}\n",
    "\n",
    "# Iterate through each row of the dataframe and find the corresponding census tract IDs for both pickup and dropoff\n",
    "for index, row in df_no_census_tract_both.iterrows():\n",
    "    pickup_location = row['pickup_centroid_location']\n",
    "    dropoff_location = row['pickup_centroid_location']\n",
    "\n",
    "    if pickup_location not in pickup_census_tract_ids:\n",
    "        pickup_census_tract_ids[pickup_location] = find_census_tract(pickup_location, df_census_tracts)\n",
    "\n",
    "    if dropoff_location not in dropoff_census_tract_ids:\n",
    "        dropoff_census_tract_ids[dropoff_location] = find_census_tract(dropoff_location, df_census_tracts)\n",
    "\n",
    "# Update the \"pickup_census_tract\" column\n",
    "df_no_census_tract_both['pickup_census_tract'] = df_no_census_tract_both['pickup_centroid_location'].map(pickup_census_tract_ids)\n",
    "\n",
    "# Update the \"dropoff_census_tract\" column\n",
    "df_no_census_tract_both['dropoff_census_tract'] = df_no_census_tract_both['pickup_centroid_location'].map(dropoff_census_tract_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16170126"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_census_not_nan = df_cleaned_census_and_location.dropna(column_names=[\"pickup_census_tract\", \"dropoff_census_tract\"])\n",
    "df_census_not_nan = vaex.from_pandas(df_census_not_nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inserted_census_tracts = df_census_not_nan.concat(df_no_census_tract_both)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check which values contain NA and NaN values\n",
    "column_names = df_inserted_census_tracts.get_column_names()\n",
    "\n",
    "for column in column_names:\n",
    "    if df_inserted_census_tracts[column].isna().sum() > 0:\n",
    "        print(f\"Column '{column}' contains NA or NaN values with a number of \" + str(df_inserted_census_tracts[column].isna().sum()) + \" rows.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We decide to drop all columns with NA or NaN values for consistent analysis across different tasks\n",
    "print(\"Total number of rows before dropping NA and NaN: \" + str(df_inserted_census_tracts.count()))\n",
    "\n",
    "df_no_missing_values = df_inserted_census_tracts.dropnan()\n",
    "print(\"Number of rows without NaN-values: \" + str(df_no_missing_values.count()))\n",
    "\n",
    "df_no_missing_values = df_no_missing_values.dropna()\n",
    "print(\"Number of rows without NA-values: \" + str(df_no_missing_values.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if trip ids are unique\n",
    "print(\"Trip IDs are unique?: \" + str(len(df_no_missing_values) == df_no_missing_values['trip_id'].nunique()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for consistency in community areas\n",
    "print(\"Number of community areas: \" + str(df_community_areas.count()))\n",
    "print(\"Number of community areas in taxi trip data without NaN-values: \" + str(df_no_missing_values.pickup_community_area.nunique(dropnan=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for consistency in community areas\n",
    "community_areas = df_community_areas.AREA_NUMBE.values.unique()\n",
    "community_areas_int = set([area.as_py() for area in community_areas])\n",
    "\n",
    "community_areas_pickup = df_no_missing_values.pickup_community_area.unique(dropnan=True)\n",
    "community_areas_pickup_int = set([int(area) for area in community_areas_pickup])\n",
    "\n",
    "community_areas_dropoff = df_no_missing_values.pickup_community_area.unique(dropnan=True)\n",
    "community_areas_dropoff_int = set([int(area) for area in community_areas_dropoff])\n",
    "\n",
    "print(\"Do the pickup community area IDs in the taxi trip data match the community area dataset? \",community_areas_pickup_int.issubset(community_areas_int))\n",
    "print(\"Do the dropoff community area IDs in the taxi trip data match the community area dataset? \",community_areas_dropoff_int.issubset(community_areas_int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for consistency in census tracts\n",
    "print(\"Number of census tracts: \" + str(df_census_tracts.count()))\n",
    "print(\"Number of pickup census tracts in filtered taxi trip data: \" + str(df_no_missing_values.pickup_census_tract.nunique()))\n",
    "print(\"Number of dropoff census tracts in filtered taxi trip data: \" + str(df_no_missing_values.dropoff_census_tract.nunique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the census tracts in the taxi data match the census tracts dataset\n",
    "df_census_tracts.GEOID10.values\n",
    "census_tracts = set([id.as_py() for id in df_census_tracts.GEOID10.values])\n",
    "census_tracts_taxi_pickups = set([int(id) for id in df_no_missing_values.pickup_census_tract.unique(dropnan=True)])\n",
    "census_tracts_taxi_dropoffs = set([int(id) for id in df_no_missing_values.dropoff_census_tract.unique(dropnan=True)])\n",
    "\n",
    "print(\"Do the pickup census tract IDs in the taxi trip data match the census tract dataset? \",census_tracts_taxi_pickups.issubset(census_tracts))\n",
    "print(\"Do the dropoff census tract IDs in the taxi trip data match the census tract dataset? \",census_tracts_taxi_dropoffs.issubset(census_tracts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create columns for hourly discretization\n",
    "df_no_missing_values[\"trip_start_hour\"] = df_no_missing_values.trip_start_timestamp.dt.hour\n",
    "df_no_missing_values[\"trip_end_hour\"] = df_no_missing_values.trip_end_timestamp.dt.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "export(hdf5) [#############---------------------------] 33.33% estimated time:     0.44s =  0.0m =  0.0h "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "export(hdf5) [########################################] 100.00% elapsed time  :  1904.98s =  31.7m =  0.5h                                                 \n",
      " "
     ]
    }
   ],
   "source": [
    "# export prepared dataframe\n",
    "df_no_missing_values.export_hdf5('./data/trips_prepared.hdf5', progress=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
