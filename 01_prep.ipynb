{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from shapely.geometry import Point, MultiPolygon\n",
    "import vaex\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pyarrow as pa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_taxi_trips_all = vaex.open('./data/trips.hdf5')\n",
    "\n",
    "### Replace spaces and uppercases in column names\n",
    "column_names = df_taxi_trips_all.column_names\n",
    "column_names_refactored = [ln.replace(' ', '_').lower() for ln in column_names]\n",
    "\n",
    "for i, column in enumerate(column_names):\n",
    "    df_taxi_trips_all.rename(column, column_names_refactored[i])\n",
    "\n",
    "# correct typo\n",
    "df_taxi_trips_all.rename(\"dropoff_centroid__location\",\"dropoff_centroid_location\")\n",
    "\n",
    "# cast timestamp columns to datetime\n",
    "date_format = \"%m/%d/%Y %I:%M:%S %p\"\n",
    "def column_to_datetime(datetime_str):\n",
    "    return np.datetime64(datetime.strptime(datetime_str, date_format))\n",
    "\n",
    "df_taxi_trips_all['trip_start_timestamp'] = df_taxi_trips_all['trip_start_timestamp'].apply(column_to_datetime)\n",
    "df_taxi_trips_all['trip_end_timestamp'] = df_taxi_trips_all['trip_end_timestamp'].apply(column_to_datetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open external data: census tracts\n",
    "df_census_tracts = vaex.open('./data/chicago_census_tracts.csv')\n",
    "df_census_tracts.rename(\"the_geom\", \"geometry\")\n",
    "\n",
    "# community areas\n",
    "df_community_areas = vaex.open('./data/community_areas.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total amount of trips: 24,988,003\n"
     ]
    }
   ],
   "source": [
    "total_trips = len(df_taxi_trips_all)\n",
    "print(f\"Total amount of trips: {total_trips:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trips with trip_miles = 0: 3,003,697\n",
      "Number of trips without trip miles and same location: 1,387,241\n"
     ]
    }
   ],
   "source": [
    "# Number of trips with trip_miles = 0\n",
    "zero_trip_miles = len(df_taxi_trips_all[df_taxi_trips_all[\"trip_miles\"] == 0])\n",
    "print(f\"Number of trips with trip_miles = 0: {zero_trip_miles:,}\")\n",
    "\n",
    "# Number of trips with trip_miles = 0 and no difference in pickup and dropoff location\n",
    "zero_trip_miles_same_loc = len(df_taxi_trips_all[(df_taxi_trips_all[\"trip_miles\"] == 0) & (df_taxi_trips_all[\"pickup_centroid_location\"] == df_taxi_trips_all[\"dropoff_centroid_location\"])])\n",
    "print(f\"Number of trips without trip miles and same location: {zero_trip_miles_same_loc:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Trips with Zero Trip Miles and Same Pickup/Dropoff Locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Trips with Non-Zero Trip Miles and Different Pickup/Dropoff Locations: 23,600,762\n"
     ]
    }
   ],
   "source": [
    "# drop rows without trip miles and same location\n",
    "df_non_zero_trip_miles = df_taxi_trips_all[(df_taxi_trips_all[\"trip_miles\"] != 0) | (df_taxi_trips_all[\"pickup_centroid_location\"] != df_taxi_trips_all[\"dropoff_centroid_location\"])]\n",
    "print(f\"Total Trips with Non-Zero Trip Miles and Different Pickup/Dropoff Locations: {len(df_non_zero_trip_miles):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insert Missing Census Tract IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of missing pickup_census_tract IDs: 30.76%\n",
      "Percentage of missing pickup_census_tract IDs: 31.13%\n"
     ]
    }
   ],
   "source": [
    "# Calculate the percentage of missing pickup_census_tract IDs\n",
    "percentage_missing_pickup_census_tract = (len(df_non_zero_trip_miles[df_non_zero_trip_miles[\"pickup_census_tract\"].isnan()]) / len(df_non_zero_trip_miles)) * 100\n",
    "percentage_missing_dropoff_census_tract = (len(df_non_zero_trip_miles[df_non_zero_trip_miles[\"dropoff_census_tract\"].isnan()]) / len(df_non_zero_trip_miles)) * 100\n",
    "\n",
    "# Round the percentage to two decimal places\n",
    "percentage_rounded_pickup = round(percentage_missing_pickup_census_tract, 2)\n",
    "percentage_rounded_dropoff = round(percentage_missing_dropoff_census_tract, 2)\n",
    "\n",
    "# Display the result\n",
    "print(f\"Percentage of missing pickup_census_tract IDs: {percentage_rounded_pickup}%\")\n",
    "print(f\"Percentage of missing pickup_census_tract IDs: {percentage_rounded_dropoff}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Trips without Rows with Null Values in \"X_centroid_location\" AND \"X_census_tract\": 21,170,643\n"
     ]
    }
   ],
   "source": [
    "# Drop all rows where both \"X_census_tract\" and \"X_centroid_location\" are null\n",
    "# We keep rows WITH \"X_centroid_location\" and WITHOUT \"pickup_census_tract\" to craft census tracts\n",
    "df_cleaned_census_and_location = df_non_zero_trip_miles.dropna(column_names=[\"pickup_census_tract\", \"pickup_centroid_location\"], how=\"all\")\n",
    "df_cleaned_census_and_location = df_cleaned_census_and_location.dropna(column_names=[\"dropoff_census_tract\", \"dropoff_centroid_location\"], how=\"all\")\n",
    "print(f\"Total Trips without Rows with Null Values in \\\"X_centroid_location\\\" AND \\\"X_census_tract\\\": {len(df_cleaned_census_and_location):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Rows where \"X_census_tract\" is Null: 5,000,517\n"
     ]
    }
   ],
   "source": [
    "df_no_census_tract_both = df_cleaned_census_and_location[df_cleaned_census_and_location[\"pickup_census_tract\"].isnan() | df_cleaned_census_and_location[\"dropoff_census_tract\"].isnan()]\n",
    "print(f\"Number of Rows where \\\"X_census_tract\\\" is Null: {len(df_no_census_tract_both):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_no_census_tract_both[:100].export(\"./data/df_missing_census_tract.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Assuming you have one dataframe containing null values for both pickup_census_tract and dropoff_census_tract\n",
    "# df_no_census_tract_both = df_no_census_tract_both.to_pandas_df()\n",
    "\n",
    "# # Convert the pickup_centroid_location in the dataframe to Point geometries\n",
    "# df_no_census_tract_both['pickup_centroid_location'] = df_no_census_tract_both.apply(\n",
    "#     lambda row: Point(row['pickup_centroid_longitude'], row['pickup_centroid_latitude']), axis=1\n",
    "# )\n",
    "\n",
    "# # Prepare a function to find the census tract for a given point\n",
    "# def find_census_tract(point, census_tract_df):\n",
    "#     for index, row in census_tract_df.iterrows():\n",
    "#         if point.within(row['geometry']):\n",
    "#             return row['GEOID10']\n",
    "#     return None\n",
    "\n",
    "# # Create dictionaries to store the census tract IDs for pickup and dropoff points\n",
    "# pickup_census_tract_ids = {}\n",
    "# dropoff_census_tract_ids = {}\n",
    "\n",
    "# # Iterate through each row of the dataframe and find the corresponding census tract IDs for both pickup and dropoff\n",
    "# for index, row in df_no_census_tract_both.iterrows():\n",
    "#     pickup_location = row['pickup_centroid_location']\n",
    "#     dropoff_location = row['pickup_centroid_location']\n",
    "\n",
    "#     if pickup_location not in pickup_census_tract_ids:\n",
    "#         pickup_census_tract_ids[pickup_location] = find_census_tract(pickup_location, df_census_tracts)\n",
    "\n",
    "#     if dropoff_location not in dropoff_census_tract_ids:\n",
    "#         dropoff_census_tract_ids[dropoff_location] = find_census_tract(dropoff_location, df_census_tracts)\n",
    "\n",
    "# # Update the \"pickup_census_tract\" column\n",
    "# df_no_census_tract_both['pickup_census_tract'] = df_no_census_tract_both['pickup_centroid_location'].map(pickup_census_tract_ids)\n",
    "\n",
    "# # Update the \"dropoff_census_tract\" column\n",
    "# df_no_census_tract_both['dropoff_census_tract'] = df_no_census_tract_both['pickup_centroid_location'].map(dropoff_census_tract_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_census_not_nan = df_cleaned_census_and_location.dropna(column_names=[\"pickup_census_tract\", \"dropoff_census_tract\"])\n",
    "# df_census_not_nan = vaex.from_pandas(df_census_not_nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_inserted_census_tracts = df_census_not_nan.concat(df_no_census_tract_both)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column 'trip_id' contains NA values with a number of 0 rows.\n",
      "Column 'taxi_id' contains NA values with a number of 7068 rows.\n",
      "Column 'trip_seconds' contains NA values with a number of 485 rows.\n",
      "Column 'trip_miles' contains NA values with a number of 406 rows.\n",
      "Column 'pickup_census_tract' contains NA values with a number of 5000517 rows.\n",
      "Column 'dropoff_census_tract' contains NA values with a number of 5000517 rows.\n",
      "Column 'pickup_community_area' contains NA values with a number of 6408 rows.\n",
      "Column 'dropoff_community_area' contains NA values with a number of 87664 rows.\n",
      "Column 'fare' contains NA values with a number of 412 rows.\n",
      "Column 'tips' contains NA values with a number of 412 rows.\n",
      "Column 'tolls' contains NA values with a number of 5482410 rows.\n",
      "Column 'extras' contains NA values with a number of 412 rows.\n",
      "Column 'trip_total' contains NA values with a number of 412 rows.\n",
      "Column 'payment_type' contains NA values with a number of 0 rows.\n",
      "Column 'company' contains NA values with a number of 0 rows.\n",
      "Column 'pickup_centroid_latitude' contains NA values with a number of 4978 rows.\n",
      "Column 'pickup_centroid_longitude' contains NA values with a number of 4978 rows.\n"
     ]
    }
   ],
   "source": [
    "# check which values contain NA and NaN values\n",
    "column_names = df_cleaned_census_and_location.get_column_names()\n",
    "column_names.remove('trip_start_timestamp')\n",
    "column_names.remove('trip_end_timestamp')\n",
    "\n",
    "for column in column_names:\n",
    "    df_na = df_cleaned_census_and_location[df_cleaned_census_and_location[column].isna()]\n",
    "    print(f\"Column '{column}' contains NA values with a number of \" + str(len(df_na)) + \" rows.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "from shapely import wkt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df_cleaned_census_and_location[\"pickup_centroid_location\"] = df_cleaned_census_and_location[\"pickup_centroid_location\"].apply(wkt.loads)\n",
    "df = df_cleaned_census_and_location[df_cleaned_census_and_location[\"pickup_centroid_location\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot cast 'trip_start_timestamp' (of type datetime64[us]) to <class 'numpy.float64'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m gdf \u001b[39m=\u001b[39m gpd\u001b[39m.\u001b[39;49mGeoDataFrame(df, geometry\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mpickup_centroid_location\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      2\u001b[0m gdf\u001b[39m.\u001b[39mplot(markersize\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m, alpha\u001b[39m=\u001b[39m\u001b[39m0.5\u001b[39m)\n\u001b[1;32m      3\u001b[0m plt\u001b[39m.\u001b[39mxlabel(\u001b[39m\"\u001b[39m\u001b[39mLongitude\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/geopandas/geodataframe.py:138\u001b[0m, in \u001b[0;36mGeoDataFrame.__init__\u001b[0;34m(self, data, geometry, crs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    133\u001b[0m         kwargs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mcopy\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    134\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(data, DataFrame)\n\u001b[1;32m    135\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(data, GeoDataFrame)\n\u001b[1;32m    136\u001b[0m     ):\n\u001b[1;32m    137\u001b[0m         kwargs\u001b[39m.\u001b[39mupdate(copy\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m--> 138\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(data, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    140\u001b[0m \u001b[39m# set_geometry ensures the geometry data have the proper dtype,\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[39m# but is not called if `geometry=None` ('geometry' column present\u001b[39;00m\n\u001b[1;32m    142\u001b[0m \u001b[39m# in the data), so therefore need to ensure it here manually\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    146\u001b[0m \n\u001b[1;32m    147\u001b[0m \u001b[39m# if gdf passed in and geo_col is set, we use that for geometry\u001b[39;00m\n\u001b[1;32m    148\u001b[0m \u001b[39mif\u001b[39;00m geometry \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(data, GeoDataFrame):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/frame.py:772\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    769\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(data, abc\u001b[39m.\u001b[39mSequence):\n\u001b[1;32m    770\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(data, \u001b[39m\"\u001b[39m\u001b[39m__array__\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    771\u001b[0m         \u001b[39m# GH#44616 big perf improvement for e.g. pytorch tensor\u001b[39;00m\n\u001b[0;32m--> 772\u001b[0m         data \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49masarray(data)\n\u001b[1;32m    773\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    774\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(data)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/vaex/dataframe.py:6220\u001b[0m, in \u001b[0;36mDataFrameLocal.__array__\u001b[0;34m(self, dtype, parallel)\u001b[0m\n\u001b[1;32m   6218\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m np\u001b[39m.\u001b[39mcan_cast(column_type, dtype):\n\u001b[1;32m   6219\u001b[0m         \u001b[39mif\u001b[39;00m column_type \u001b[39m!=\u001b[39m dtype:\n\u001b[0;32m-> 6220\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCannot cast \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m (of type \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m) to \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (name, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata_type(name), dtype))\n\u001b[1;32m   6221\u001b[0m chunks \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mevaluate(column_names, parallel\u001b[39m=\u001b[39mparallel, array_type\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mnumpy\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m   6222\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39many\u001b[39m(np\u001b[39m.\u001b[39mma\u001b[39m.\u001b[39misMaskedArray(chunk) \u001b[39mfor\u001b[39;00m chunk \u001b[39min\u001b[39;00m chunks):\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot cast 'trip_start_timestamp' (of type datetime64[us]) to <class 'numpy.float64'>"
     ]
    }
   ],
   "source": [
    "gdf = gpd.GeoDataFrame(df, geometry=\"pickup_centroid_location\")\n",
    "gdf.plot(markersize=5, alpha=0.5)\n",
    "plt.xlabel(\"Longitude\")\n",
    "plt.ylabel(\"Latitude\")\n",
    "plt.title(\"Scatter Plot of Pickup Locations (Geopandas)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of rows before dropping NA: 21170643\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows without NA-values: 12263866\n"
     ]
    }
   ],
   "source": [
    "# We decide to drop all columns with NA or NaN values for consistent analysis across different tasks\n",
    "print(\"Total number of rows before dropping NA: \" + str(len(df_cleaned_census_and_location)))\n",
    "\n",
    "df_cleaned_all = df_cleaned_census_and_location.dropna()\n",
    "print(\"Number of rows without NA-values: \" + str(len(df_cleaned_all)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# check if trip ids are unique\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mTrip IDs are unique?: \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(\u001b[39mlen\u001b[39m(df_cleaned_all) \u001b[39m==\u001b[39m \u001b[39mlen\u001b[39m(df_cleaned_all[\u001b[39m'\u001b[39;49m\u001b[39mtrip_id\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49munique())))\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/vaex/expression.py:1043\u001b[0m, in \u001b[0;36mExpression.unique\u001b[0;34m(self, dropna, dropnan, dropmissing, selection, axis, limit, limit_raise, array_type, progress, delay)\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[39m@docsubst\u001b[39m\n\u001b[1;32m   1029\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39munique\u001b[39m(\u001b[39mself\u001b[39m, dropna\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, dropnan\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, dropmissing\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, selection\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, axis\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, limit\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, limit_raise\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, array_type\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mlist\u001b[39m\u001b[39m'\u001b[39m, progress\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, delay\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m   1030\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Returns all unique values.\u001b[39;00m\n\u001b[1;32m   1031\u001b[0m \n\u001b[1;32m   1032\u001b[0m \u001b[39m    :param dropna: {dropna}\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1041\u001b[0m \u001b[39m    :param bool delay: {delay}\u001b[39;00m\n\u001b[1;32m   1042\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1043\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mds\u001b[39m.\u001b[39;49munique(\u001b[39mself\u001b[39;49m, dropna\u001b[39m=\u001b[39;49mdropna, dropnan\u001b[39m=\u001b[39;49mdropnan, dropmissing\u001b[39m=\u001b[39;49mdropmissing, selection\u001b[39m=\u001b[39;49mselection, array_type\u001b[39m=\u001b[39;49marray_type, axis\u001b[39m=\u001b[39;49maxis, limit\u001b[39m=\u001b[39;49mlimit, limit_raise\u001b[39m=\u001b[39;49mlimit_raise, progress\u001b[39m=\u001b[39;49mprogress, delay\u001b[39m=\u001b[39;49mdelay)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/vaex/dataframe.py:681\u001b[0m, in \u001b[0;36mDataFrame.unique\u001b[0;34m(self, expression, return_inverse, dropna, dropnan, dropmissing, progress, selection, axis, delay, limit, limit_raise, array_type)\u001b[0m\n\u001b[1;32m    679\u001b[0m \u001b[39mif\u001b[39;00m return_inverse:\n\u001b[1;32m    680\u001b[0m     progress_inverse \u001b[39m=\u001b[39m progressbar\u001b[39m.\u001b[39madd(\u001b[39m\"\u001b[39m\u001b[39mfind inverse\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 681\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_delay(delay, progressbar\u001b[39m.\u001b[39;49mexit_on(process(hash_map_result)))\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/vaex/dataframe.py:1780\u001b[0m, in \u001b[0;36mDataFrame._delay\u001b[0;34m(self, delay, task, progressbar)\u001b[0m\n\u001b[1;32m   1778\u001b[0m     \u001b[39mreturn\u001b[39;00m task\n\u001b[1;32m   1779\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1780\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexecute()\n\u001b[1;32m   1781\u001b[0m     \u001b[39mreturn\u001b[39;00m task\u001b[39m.\u001b[39mget()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/vaex/dataframe.py:421\u001b[0m, in \u001b[0;36mDataFrame.execute\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    419\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mrepr\u001b[39m(task))\n\u001b[1;32m    420\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexecutor\u001b[39m.\u001b[39mtasks:\n\u001b[0;32m--> 421\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexecutor\u001b[39m.\u001b[39;49mexecute()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/vaex/execution.py:308\u001b[0m, in \u001b[0;36mExecutorLocal.execute\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mexecute\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 308\u001b[0m     \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexecute_generator():\n\u001b[1;32m    309\u001b[0m         \u001b[39mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/vaex/execution.py:432\u001b[0m, in \u001b[0;36mExecutorLocal.execute_generator\u001b[0;34m(self, use_async)\u001b[0m\n\u001b[1;32m    430\u001b[0m         logger\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mPass cancelled because of the global progress event: \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignal_progress\u001b[39m.\u001b[39mcallbacks)\n\u001b[1;32m    431\u001b[0m     \u001b[39mreturn\u001b[39;00m ok_tasks \u001b[39mand\u001b[39;00m ok_executor \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m all_stopped\n\u001b[0;32m--> 432\u001b[0m \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mthread_pool\u001b[39m.\u001b[39mmap(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocess_part, dataset\u001b[39m.\u001b[39mchunk_iterator(run\u001b[39m.\u001b[39mdataset_deps, chunk_size),\n\u001b[1;32m    433\u001b[0m                                     dataset\u001b[39m.\u001b[39mrow_count,\n\u001b[1;32m    434\u001b[0m                                     progress\u001b[39m=\u001b[39mprogress,\n\u001b[1;32m    435\u001b[0m                                     cancel\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cancel(run), unpack\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, run\u001b[39m=\u001b[39mrun, use_async\u001b[39m=\u001b[39muse_async)\n\u001b[1;32m    436\u001b[0m duration_wallclock \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m t0\n\u001b[1;32m    437\u001b[0m logger\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mexecuting took \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m seconds\u001b[39m\u001b[39m\"\u001b[39m, duration_wallclock)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/vaex/multithreading.py:104\u001b[0m, in \u001b[0;36mThreadPoolIndex.map\u001b[0;34m(self, callable, iterator, count, on_error, progress, cancel, unpack, use_async, **kwargs_extra)\u001b[0m\n\u001b[1;32m    102\u001b[0m iterator \u001b[39m=\u001b[39m \u001b[39miter\u001b[39m(buffer(iterator, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_max_workers \u001b[39m+\u001b[39m \u001b[39m3\u001b[39m))\n\u001b[1;32m    103\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 104\u001b[0m     \u001b[39mfor\u001b[39;00m value \u001b[39min\u001b[39;00m iterator:\n\u001b[1;32m    105\u001b[0m         \u001b[39mif\u001b[39;00m use_async:\n\u001b[1;32m    106\u001b[0m             value \u001b[39m=\u001b[39m \u001b[39myield\u001b[39;00m value\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/vaex/itertools.py:5\u001b[0m, in \u001b[0;36mbuffer\u001b[0;34m(i, n)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m      4\u001b[0m     \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[0;32m----> 5\u001b[0m         values\u001b[39m.\u001b[39mappend(\u001b[39mnext\u001b[39;49m(i))\n\u001b[1;32m      6\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m      7\u001b[0m         values\u001b[39m.\u001b[39mappend(\u001b[39mnext\u001b[39m(i))\n",
      "File \u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py:621\u001b[0m, in \u001b[0;36mExecutor.map.<locals>.result_iterator\u001b[0;34m()\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[39mwhile\u001b[39;00m fs:\n\u001b[1;32m    619\u001b[0m     \u001b[39m# Careful not to keep a reference to the popped future\u001b[39;00m\n\u001b[1;32m    620\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 621\u001b[0m         \u001b[39myield\u001b[39;00m _result_or_cancel(fs\u001b[39m.\u001b[39;49mpop())\n\u001b[1;32m    622\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    623\u001b[0m         \u001b[39myield\u001b[39;00m _result_or_cancel(fs\u001b[39m.\u001b[39mpop(), end_time \u001b[39m-\u001b[39m time\u001b[39m.\u001b[39mmonotonic())\n",
      "File \u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py:319\u001b[0m, in \u001b[0;36m_result_or_cancel\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    318\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 319\u001b[0m         \u001b[39mreturn\u001b[39;00m fut\u001b[39m.\u001b[39;49mresult(timeout)\n\u001b[1;32m    320\u001b[0m     \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    321\u001b[0m         fut\u001b[39m.\u001b[39mcancel()\n",
      "File \u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py:453\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39m==\u001b[39m FINISHED:\n\u001b[1;32m    451\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__get_result()\n\u001b[0;32m--> 453\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_condition\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[1;32m    456\u001b[0m     \u001b[39mraise\u001b[39;00m CancelledError()\n",
      "File \u001b[0;32m/usr/lib/python3.10/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[39mtry\u001b[39;00m:    \u001b[39m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         waiter\u001b[39m.\u001b[39;49macquire()\n\u001b[1;32m    321\u001b[0m         gotit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# check if trip ids are unique\n",
    "print(\"Trip IDs are unique?: \" + str(len(df_cleaned_all) == len(df_cleaned_all['trip_id'].unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do the pickup community area IDs in the taxi trip data match the community area dataset?  True\n",
      "Do the dropoff community area IDs in the taxi trip data match the community area dataset?  True\n"
     ]
    }
   ],
   "source": [
    "# check for consistency in community areas\n",
    "community_areas = df_community_areas.AREA_NUMBE.values.unique()\n",
    "community_areas_int = set([area.as_py() for area in community_areas])\n",
    "\n",
    "community_areas_pickup = df_cleaned_all.pickup_community_area.unique(dropnan=True)\n",
    "community_areas_pickup_int = set([int(area) for area in community_areas_pickup])\n",
    "\n",
    "community_areas_dropoff = df_cleaned_all.pickup_community_area.unique(dropnan=True)\n",
    "community_areas_dropoff_int = set([int(area) for area in community_areas_dropoff])\n",
    "\n",
    "print(\"Do the pickup community area IDs in the taxi trip data match the community area dataset? \",community_areas_pickup_int.issubset(community_areas_int))\n",
    "print(\"Do the dropoff community area IDs in the taxi trip data match the community area dataset? \",community_areas_dropoff_int.issubset(community_areas_int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do the pickup census tract IDs in the taxi trip data match the census tract dataset?  True\n",
      "Do the dropoff census tract IDs in the taxi trip data match the census tract dataset?  True\n"
     ]
    }
   ],
   "source": [
    "# check if the census tracts in the taxi data match the census tracts dataset\n",
    "df_census_tracts.GEOID10.values\n",
    "census_tracts = set([id.as_py() for id in df_census_tracts.GEOID10.values])\n",
    "census_tracts_taxi_pickups = set([int(id) for id in df_cleaned_all.pickup_census_tract.unique(dropnan=True)])\n",
    "census_tracts_taxi_dropoffs = set([int(id) for id in df_cleaned_all.dropoff_census_tract.unique(dropnan=True)])\n",
    "\n",
    "print(\"Do the pickup census tract IDs in the taxi trip data match the census tract dataset? \",census_tracts_taxi_pickups.issubset(census_tracts))\n",
    "print(\"Do the dropoff census tract IDs in the taxi trip data match the census tract dataset? \",census_tracts_taxi_dropoffs.issubset(census_tracts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create columns for hourly discretization\n",
    "df_cleaned_all[\"trip_start_hour\"] = df_cleaned_all.trip_start_timestamp.dt.hour\n",
    "df_cleaned_all[\"trip_end_hour\"] = df_cleaned_all.trip_end_timestamp.dt.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12263866\n"
     ]
    }
   ],
   "source": [
    "print(len(df_cleaned_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "export(hdf5) [#############---------------------------] 33.33% estimated time:     0.44s =  0.0m =  0.0h export(hdf5) [########################################] 100.00% elapsed time  :  1904.98s =  31.7m =  0.5h                                                 \n",
      " "
     ]
    }
   ],
   "source": [
    "# export prepared dataframe\n",
    "df_cleaned_all.export_hdf5('./data/trips_prepared.hdf5', progress=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
