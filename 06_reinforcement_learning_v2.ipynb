{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce746fcf-eb82-4a35-ba41-b51b982d6662",
   "metadata": {},
   "source": [
    "# Smart Charging Using Reinforcement Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d6e09303",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gym import Env\n",
    "from gym.spaces import Box, Discrete\n",
    "import random\n",
    "import math\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "from rl.agents import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "import matplotlib.pyplot as plt \n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "#%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95bd7b5",
   "metadata": {},
   "source": [
    "## Electric Vehicle Charging Environment\n",
    "We start by creating the environment for simulating electric vehicle (EV) charging using the OpenAI Gym framework. The environment defines a Markov decision process, including states, actions and a reward function with different charging rates and time-dependent energy prices.\n",
    "\n",
    "### \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69fca89f",
   "metadata": {},
   "source": [
    "## Electric Vehicle Charging Environment\n",
    "\n",
    "This code defines a custom environment for simulating electric vehicle (EV) charging using the OpenAI Gym framework. The environment models the charging process of an EV with different charging rates and time-dependent energy prices.\n",
    "\n",
    "### Key Components\n",
    "\n",
    "- `action_space`: The environment offers four charging actions: no charge, low, medium, and high charging.\n",
    "\n",
    "- `battery_limit`: Represents the EV's battery capacity, assumed to be 100 kWh.\n",
    "\n",
    "- `battery_level`: Initialized to the battery's full capacity at the start.\n",
    "\n",
    "- `observation_space`: A state consists of the battery level and the current time interval.\n",
    "\n",
    "- `current_time_index`: Tracks the time interval (0-7) for charging intervals.\n",
    "\n",
    "- `time_coefficients`: Coefficients representing volatile energy prices from 2pm to 4pm ranging from 47$/MWh to 50$/MWh or 0.94 to 1. (Source: https://www.researchgate.net/profile/Torjus-Bolkesjo/publication/310021998/figure/fig3/AS:431952714047493@1479996960479/Hourly-intra-day-variation-of-the-electricity-price-for-Germany-in-MWh-and-the.png)\n",
    "\n",
    "- `charging_rates`: Charging rates for different actions (0, 1.75, 3.5, 5.5 kWh).\n",
    "\n",
    "### Functions\n",
    "\n",
    "- `step(action)`: Simulates one time step in the environment. The agent's action determines the charging rate, and the environment updates the battery level, calculates rewards based on energy prices, and applies penalties for insufficient battery levels.\n",
    "\n",
    "- `reset()`: Resets the environment to the initial state, including battery level and time.\n",
    "\n",
    "### Objective\n",
    "\n",
    "The agent's objective is to make charging decisions that minimize costs, considering time of day and charging rate. By interacting with this environment, the agent can learn strategies to optimize the charging process and manage the EV's battery effectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "70646c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EVChargingEnvironment(Env):\n",
    "    def __init__(self):\n",
    "        self.action_space = Discrete(4) # Action space contains 4 actions: zero, low, medium, high charging\n",
    "        self.battery_limit = 100 # Assumption: EV has capacity of 100 kWh\n",
    "        self.battery_level = self.battery_limit # Assumption: Battery fully charged at initialization\n",
    "\n",
    "        # A state in the environment consists of battery level (0-100 kWh) and time interval (0-7) \n",
    "        self.observation_space = np.array([Box(low=np.array([0]), high = np.array([self.battery_limit])), Box(low=np.array([0]), high = np.array([8]))])\n",
    "\n",
    "        # Initialize first state\n",
    "        self.current_time_index = 0\n",
    "        self.state = np.array([self.battery_level, self.current_time_index]) \n",
    "\n",
    "        # Electricity price distribution: Starting at $50/MWh, decreasing to $47/MWh after 4 intervals, then increasing back to $50/MWh\n",
    "        self.time_coefficients = [1.0, 0.98, 0.96, 0.94, 0.94, 0.96, 0.98, 1.0]\n",
    "\n",
    "        # Charging rates in range 0, 7, 14, 22 kWh divided by 4 (because we charge only 15 minutes)\n",
    "        self.charging_rates = [0, 1.75, 3.5, 5.5]\n",
    "\n",
    "    def step(self, action):\n",
    "\n",
    "        # Set time attribute of state to \n",
    "        self.state[1] = self.current_time_index\n",
    "\n",
    "        # Get current time coefficient and charging rate\n",
    "        time_coefficient = self.time_coefficients[self.current_time_index]\n",
    "        charging_rate = self.charging_rates[action]\n",
    "        \n",
    "        # If the charging rate exceeds the limit, we only charge the amount of difference\n",
    "        if self.battery_level + charging_rate > self.battery_limit:\n",
    "            charging_rate = self.battery_limit - self.battery_level\n",
    "        \n",
    "        # Increase battery level\n",
    "        self.battery_level += charging_rate\n",
    "        self.state[0] = self.battery_level\n",
    "        \n",
    "        # Reward function\n",
    "        reward = time_coefficient * math.exp(charging_rate) * (-1)\n",
    "\n",
    "        # Assumption: Reward should be Zero for Zero charging\n",
    "        if charging_rate == 0:\n",
    "            reward = 0\n",
    "\n",
    "        # Update time index\n",
    "        self.current_time_index = (self.current_time_index + 1) % 8\n",
    "\n",
    "        # Check if simulation is complete\n",
    "        done = self.current_time_index == 0\n",
    "\n",
    "        if done:\n",
    "\n",
    "            # Set energy demand\n",
    "            energy_demand = np.random.normal(30, 5)\n",
    "\n",
    "            # Set new battery level\n",
    "            self.battery_level = self.battery_level - energy_demand\n",
    "            \n",
    "            # Set penalty if battery level is too low for energy demand\n",
    "            if self.battery_level < 0:\n",
    "                # We set a very high penalty of 10,000 because of the cost function with prices up to 244\n",
    "                reward -= 10000.0\n",
    "                self.battery_level = 0 # Set Battery Level to Zero\n",
    "                self.state[0] = self.battery_level\n",
    "\n",
    "        # Setting the placeholder for info\n",
    "        info = {}\n",
    "        \n",
    "        return self.state, reward, done, info\n",
    "\n",
    "    def reset(self):\n",
    "\n",
    "        # Set current time index to Zero\n",
    "        self.current_time_index = 0\n",
    "\n",
    "        # Initialize first state of episode\n",
    "        # We use the current battery level resulting from the last episode (after demand subtraction)\n",
    "        self.state = np.array([self.battery_level, self.current_time_index])\n",
    "\n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "1fff4f43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lukas/.local/lib/python3.10/site-packages/gym/spaces/box.py:127: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n"
     ]
    }
   ],
   "source": [
    "env = EVChargingEnvironment()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c44e2bd",
   "metadata": {},
   "source": [
    "### Environment Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c06bbdd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = 10\n",
    "\n",
    "for episode in range(1, episodes+1):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "\n",
    "    while not done:\n",
    "        action = env.action_space.sample()\n",
    "        n_state, reward, done, info = env.step(action)\n",
    "        score += reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "98096b85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 10\n",
      "Initial State: [0 0]\n",
      "Action: 1 | New State: [1 0] | Reward: -5.754602676005731 | Done: False\n",
      "Action: 2 | New State: [5 1] | Reward: -32.45314291951846 | Done: False\n",
      "Action: 0 | New State: [5 2] | Reward: 0 | Done: False\n",
      "Action: 2 | New State: [8 3] | Reward: -31.128524841170773 | Done: False\n",
      "Action: 0 | New State: [8 4] | Reward: 0 | Done: False\n",
      "Action: 3 | New State: [14  5] | Reward: -234.90425497365155 | Done: False\n",
      "Action: 1 | New State: [16  6] | Reward: -5.639510622485616 | Done: False\n",
      "Action: 0 | New State: [0 7] | Reward: -10000.0 | Done: True\n",
      "Episode 10 - Score: -10309.880036032831\n",
      "**************************************************\n"
     ]
    }
   ],
   "source": [
    "episodes = 10\n",
    "\n",
    "for episode in range(1, episodes+1):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "\n",
    "    if episode % 10 == 0:\n",
    "        print(f\"Episode {episode}\\nInitial State: {state}\")\n",
    "\n",
    "    while not done:\n",
    "        action = env.action_space.sample()\n",
    "        n_state, reward, done, info = env.step(action)\n",
    "        score += reward\n",
    "\n",
    "        if episode % 10 == 0:\n",
    "            print(f\"Action: {action} | New State: {n_state} | Reward: {reward} | Done: {done}\")\n",
    "    \n",
    "    if episode % 10 == 0:\n",
    "        print(f\"Episode {episode} - Score: {score}\\n{'*' * 50}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4df2b4",
   "metadata": {},
   "source": [
    "We can see how the agent performs actions in the environment and gets the rewards. Without a learnt policy, the agent is not able to meet the energy demand. Therefore, the rewards suffer from high penalties."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f084755e",
   "metadata": {},
   "source": [
    "## Deep Q-Network using Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1267938",
   "metadata": {},
   "source": [
    "In the next step we create a deep Q-Network for approximating the Q-values in our environment. With Q-Learning we are able to learn about the greedy policy, while using a different behaviour policy for acting in the environment. The behaviour policy is usually an e-greedy policy, that selects the greedy action with probability 1 - e and a random action with probability e to ensure good coverage of the state-action space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "bab62b7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actions: 4 | States: (2,)\n"
     ]
    }
   ],
   "source": [
    "# Get shpaes of states and actions of our environment\n",
    "states = env.observation_space.shape\n",
    "actions = env.action_space.n\n",
    "print(\"Actions: \" + str(actions) + \" | States: \" + str(states))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "0c7b481e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the model for Q- and target network. We define one flatten layer and three dense layers with ReLU activation function\n",
    "def build_model(states, actions):\n",
    "    model = Sequential()   \n",
    "    model.add(Flatten(input_shape=(1,2)))\n",
    "    model.add(Dense(24, activation='relu'))\n",
    "    model.add(Dense(24, activation='relu'))\n",
    "    model.add(Dense(actions, activation='linear'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "d9d37555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate model\n",
    "model = build_model(states, actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e436c801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_2 (Flatten)         (None, 2)                 0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 24)                72        \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 24)                600       \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 4)                 100       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 772 (3.02 KB)\n",
      "Trainable params: 772 (3.02 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45df08f3",
   "metadata": {},
   "source": [
    "**build_agent**: In this function, we define our policy using the **BoltzmannQPolicy**. This policy constructs a probability distribution based on the Q-values and randomly selects an action according to this distribution. The DQNAgent uses Sequential Memory to store various states, actions and rewards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "def50195",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_agent(model, actions):\n",
    "    policy = BoltzmannQPolicy()\n",
    "    memory = SequentialMemory(limit=50000, window_length=1)\n",
    "    dqn = DQNAgent(model=model, memory=memory, policy=policy, \n",
    "                  nb_actions=actions, nb_steps_warmup=10000, target_model_update=1e-2)\n",
    "    return dqn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36938fb2",
   "metadata": {},
   "source": [
    "Now we can create an agent that learns a policy through minimizing the loss between the target Q-network (actual Q-values) and the Q-network (approximated Q-values). We employ the Adam optimizer with a learning rate of 0.001 and use mean squared error (MSE) as the loss function. The agent is trained for 60,000 steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "0a059b3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-13 23:56:56.005574: W tensorflow/c/c_api.cc:304] Operation '{name:'dense_12_2/kernel/Assign' id:3492 op device:{requested: '', assigned: ''} def:{{{node dense_12_2/kernel/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](dense_12_2/kernel, dense_12_2/kernel/Initializer/stateless_random_uniform)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 60000 steps ...\n",
      "Interval 1 (0 steps performed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-13 23:56:56.425711: W tensorflow/c/c_api.cc:304] Operation '{name:'dense_14/BiasAdd' id:2831 op device:{requested: '', assigned: ''} def:{{{node dense_14/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format=\"NHWC\"](dense_14/MatMul, dense_14/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2023-08-13 23:56:56.519529: W tensorflow/c/c_api.cc:304] Operation '{name:'total_20/Assign' id:3673 op device:{requested: '', assigned: ''} def:{{{node total_20/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](total_20, total_20/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 22s 2ms/step - reward: -1039.8591\n",
      "1250 episodes - episode_reward: -8318.873 [-11429.001, -139.767]\n",
      "\n",
      "Interval 2 (10000 steps performed)\n",
      "    1/10000 [..............................] - ETA: 26s - reward: -244.6919"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-13 23:57:18.791003: W tensorflow/c/c_api.cc:304] Operation '{name:'dense_14_2/BiasAdd' id:3550 op device:{requested: '', assigned: ''} def:{{{node dense_14_2/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format=\"NHWC\"](dense_14_2/MatMul, dense_14_2/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2023-08-13 23:57:19.157202: W tensorflow/c/c_api.cc:304] Operation '{name:'loss_25/AddN' id:3804 op device:{requested: '', assigned: ''} def:{{{node loss_25/AddN}} = AddN[N=2, T=DT_FLOAT, _has_manual_control_dependencies=true](loss_25/mul, loss_25/mul_1)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2023-08-13 23:57:19.281826: W tensorflow/c/c_api.cc:304] Operation '{name:'training_8/Adam/dense_13/kernel/v/Assign' id:4004 op device:{requested: '', assigned: ''} def:{{{node training_8/Adam/dense_13/kernel/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_8/Adam/dense_13/kernel/v, training_8/Adam/dense_13/kernel/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 170s 17ms/step - reward: -1124.9075\n",
      "1250 episodes - episode_reward: -8999.260 [-11257.016, -145.176] - loss: 1949687.805 - mae: 4845.962 - mean_q: -5891.580\n",
      "\n",
      "Interval 3 (20000 steps performed)\n",
      "10000/10000 [==============================] - 205s 20ms/step - reward: -1107.0046\n",
      "1250 episodes - episode_reward: -8856.037 [-11664.651, -112.608] - loss: 719627.750 - mae: 4804.793 - mean_q: -5759.393\n",
      "\n",
      "Interval 4 (30000 steps performed)\n",
      "10000/10000 [==============================] - 176s 18ms/step - reward: -701.2505\n",
      "1250 episodes - episode_reward: -5610.004 [-11220.994, -135.134] - loss: 648592.250 - mae: 4303.357 - mean_q: -4984.023\n",
      "\n",
      "Interval 5 (40000 steps performed)\n",
      "10000/10000 [==============================] - 179s 18ms/step - reward: -578.7039\n",
      "1250 episodes - episode_reward: -4629.631 [-11226.403, -114.365] - loss: 574162.000 - mae: 3931.816 - mean_q: -4426.032\n",
      "\n",
      "Interval 6 (50000 steps performed)\n",
      "10000/10000 [==============================] - 186s 19ms/step - reward: -1102.7549\n",
      "done, took 938.268 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7ff2f08d98a0>"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn = build_agent(model, actions)\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
    "dqn.fit(env, nb_steps=60000, visualize=False, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5331d4d9",
   "metadata": {},
   "source": [
    "### Testing EV Charging Environment\n",
    "After training our policy, we now can test it and see the actions taken by the learnt policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "947ecd01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: Initial battery level: 41.91486840115341\n",
      "Episode 1: Learned Policy: [0, 0, 3, 3, 3, 3, 3, 3]\n",
      "Episode 1: Score: -1414.3193684871937\n",
      "Episode 101: Initial battery level: 51.39121492180952\n",
      "Episode 101: Learned Policy: [0, 0, 2, 3, 3, 3, 3, 3]\n",
      "Episode 101: Score: -1211.2059473938868\n",
      "Episode 201: Initial battery level: 35.77948513130855\n",
      "Episode 201: Learned Policy: [3, 2, 3, 3, 3, 3, 3, 3]\n",
      "Episode 201: Score: -1691.4644436709327\n",
      "Episode 301: Initial battery level: 39.26949129690014\n",
      "Episode 301: Learned Policy: [2, 0, 3, 3, 3, 3, 3, 3]\n",
      "Episode 301: Score: -1447.434820445886\n",
      "Episode 401: Initial battery level: 52.98116246325257\n",
      "Episode 401: Learned Policy: [0, 0, 0, 3, 3, 3, 3, 3]\n",
      "Episode 401: Score: -1179.4151135135423\n",
      "Episode 501: Initial battery level: 51.29814213687518\n",
      "Episode 501: Learned Policy: [0, 0, 2, 3, 3, 3, 3, 3]\n",
      "Episode 501: Score: -1211.2059473938868\n",
      "Episode 601: Initial battery level: 43.7825279952645\n",
      "Episode 601: Learned Policy: [0, 0, 3, 3, 3, 3, 3, 3]\n",
      "Episode 601: Score: -1414.3193684871937\n",
      "Episode 701: Initial battery level: 43.43090353756385\n",
      "Episode 701: Learned Policy: [0, 0, 3, 3, 3, 3, 3, 3]\n",
      "Episode 701: Score: -1414.3193684871937\n",
      "Episode 801: Initial battery level: 51.64460680180499\n",
      "Episode 801: Learned Policy: [0, 0, 0, 3, 3, 3, 3, 3]\n",
      "Episode 801: Score: -1179.4151135135423\n",
      "Episode 901: Initial battery level: 48.60342564631135\n",
      "Episode 901: Learned Policy: [0, 0, 2, 3, 3, 3, 3, 3]\n",
      "Episode 901: Score: -1211.2059473938868\n"
     ]
    }
   ],
   "source": [
    "num_episodes = 1000\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    learned_policy = []\n",
    "    scores = 0\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    battery_level = state[0]\n",
    "    if episode % 100 == 0:\n",
    "        print(f\"Episode {episode + 1}: Initial battery level: {battery_level}\") \n",
    "\n",
    "    while not done:\n",
    "        state = np.expand_dims(state, axis=0)\n",
    "        q_values = dqn.compute_q_values(state)  # Get Q-values from the DQN\n",
    "        learned_action = np.argmax(q_values)  # Choose action with highest Q-value\n",
    "        learned_policy.append(learned_action)\n",
    "        state, reward, done, _ = env.step(learned_action)\n",
    "        scores += reward\n",
    "    if episode % 100 == 0:\n",
    "        print(f\"Episode {episode + 1}: Learned Policy: {learned_policy}\")\n",
    "        print(f\"Episode {episode + 1}: Score: {scores}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38fdce4",
   "metadata": {},
   "source": [
    "From these results we can obtain that the optimal charging policy is to avoid charging in the first two intervals. We know from the energy price market, that energy prices are high at this time. The agent uses the lower energy prices in the middle of the time period (around 3pm). The agent has also learnt to charge with a higher rate when the initial battery level is low. Therefore, it avoids that the vehicle runs out of energy.\n",
    "\n",
    "A possible reason for the agent to charge with a high charging rate at the end of the episode is to avoid the penalty for running out of energy. Another reason could be the discount factor used in the calculation of the target Q-values. The discount factor makes the agent prioritize immediate rewards over delayed rewards."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
