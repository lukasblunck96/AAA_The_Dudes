{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "\n",
    "### Previous demand as input\n",
    "\n",
    "As we have given time series data, it is a common approach to use the demand of previous hours (or days etc.) as an input for the prediction. The assumption we hereby make is that the factors that influence the demand have not changed dramatically within the used time frames. We have decided to construct the following features from previous demand:\n",
    "\n",
    "* 2 hour: The asssumption is that the demand should not change dramatically between three hours.\n",
    "* 24 hours: The asssumption is that the current demand should be comparable to the demand exactly one day ago, as factors such as season, time of the day are the same.\n",
    "* Average demand of the past week at the same day time: This feature is the average of all 7 demand observations of the past week at same time of the day. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vaex\n",
    "import h3\n",
    "import pandas as pd\n",
    "\n",
    "df_taxi_trips = vaex.open('./data/trips_prepared.hdf5')\n",
    "df_taxi_trips.head()\n",
    "\n",
    "df_taxi_trips[\"trip_start_day\"] = df_taxi_trips.trip_start_timestamp.dt.day\n",
    "df_taxi_trips[\"trip_start_month\"] = df_taxi_trips.trip_start_timestamp.dt.month\n",
    "df_taxi_trips[\"trip_start_hour\"] = df_taxi_trips.trip_start_timestamp.dt.hour\n",
    "df_taxi_trips[\"trip_start_minute\"] = df_taxi_trips.trip_start_timestamp.dt.minute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESOLUTION = 10\n",
    "def geo_to_h3(row1, row2):\n",
    "    return h3.geo_to_h3(row1,row2, RESOLUTION)\n",
    "\n",
    "# Step 1: For each pickup and drop-off calculate the correct hexagon in the resolution\n",
    "df_taxi_trips['pickup_hex'] = df_taxi_trips.apply(geo_to_h3, [df_taxi_trips['pickup_centroid_latitude'], df_taxi_trips['pickup_centroid_longitude']])\n",
    "df_taxi_trips['dropoff_hex'] = df_taxi_trips.apply(geo_to_h3, [df_taxi_trips['dropoff_centroid_latitude'], df_taxi_trips['dropoff_centroid_longitude']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "### LONG LOADING TIME\n",
    "df_demand = df_taxi_trips.groupby(['trip_start_hour', 'trip_start_month', 'trip_start_day', 'pickup_hex']).agg({'demand': 'count'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# craft timestamp column\n",
    "df_demand['timestamp']=pd.to_datetime({'year': 2017, 'month': df_demand['trip_start_month'].to_numpy(), 'day': df_demand['trip_start_day'].to_numpy(), 'hour': df_demand['trip_start_hour'].to_numpy()}).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to pandas df\n",
    "df_demand = df_demand.to_pandas_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "MultiIndex has no single backing array. Use 'MultiIndex.to_numpy()' to get a NumPy array of tuples.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/lblunck/DEVELOPMENT/AAA_The_Dudes/05_prediction.ipynb Cell 7\u001b[0m in \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/lblunck/DEVELOPMENT/AAA_The_Dudes/05_prediction.ipynb#Y152sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m df_demand \u001b[39m=\u001b[39m df_demand\u001b[39m.\u001b[39mset_index([\u001b[39m'\u001b[39m\u001b[39mpickup_hex\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mtimestamp\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/lblunck/DEVELOPMENT/AAA_The_Dudes/05_prediction.ipynb#Y152sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m df_resampled \u001b[39m=\u001b[39m df_demand\u001b[39m.\u001b[39;49mgroupby(\u001b[39m'\u001b[39;49m\u001b[39mpickup_hex\u001b[39;49m\u001b[39m'\u001b[39;49m)\u001b[39m.\u001b[39;49mresample(\u001b[39m'\u001b[39;49m\u001b[39mH\u001b[39;49m\u001b[39m'\u001b[39;49m)\u001b[39m.\u001b[39msum()\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/lblunck/DEVELOPMENT/AAA_The_Dudes/05_prediction.ipynb#Y152sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m all_hexagons \u001b[39m=\u001b[39m df_demand\u001b[39m.\u001b[39mindex\u001b[39m.\u001b[39mget_level_values(\u001b[39m'\u001b[39m\u001b[39mpickup_hex\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39munique()\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/lblunck/DEVELOPMENT/AAA_The_Dudes/05_prediction.ipynb#Y152sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m all_hours \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mdate_range(start\u001b[39m=\u001b[39mdf_demand\u001b[39m.\u001b[39mindex\u001b[39m.\u001b[39mget_level_values(\u001b[39m'\u001b[39m\u001b[39mtimestamp\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mmin()\u001b[39m.\u001b[39mfloor(\u001b[39m'\u001b[39m\u001b[39mD\u001b[39m\u001b[39m'\u001b[39m),\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/lblunck/DEVELOPMENT/AAA_The_Dudes/05_prediction.ipynb#Y152sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m                           end\u001b[39m=\u001b[39mdf_demand\u001b[39m.\u001b[39mindex\u001b[39m.\u001b[39mget_level_values(\u001b[39m'\u001b[39m\u001b[39mtimestamp\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mmax()\u001b[39m.\u001b[39mceil(\u001b[39m'\u001b[39m\u001b[39mD\u001b[39m\u001b[39m'\u001b[39m),\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/lblunck/DEVELOPMENT/AAA_The_Dudes/05_prediction.ipynb#Y152sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m                           freq\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mH\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/groupby/groupby.py:2772\u001b[0m, in \u001b[0;36mGroupBy.resample\u001b[0;34m(self, rule, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2674\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   2675\u001b[0m \u001b[39mProvide resampling when using a TimeGrouper.\u001b[39;00m\n\u001b[1;32m   2676\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2768\u001b[0m \u001b[39m5   2000-01-01 00:03:00  5  1\u001b[39;00m\n\u001b[1;32m   2769\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   2770\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mresample\u001b[39;00m \u001b[39mimport\u001b[39;00m get_resampler_for_grouping\n\u001b[0;32m-> 2772\u001b[0m \u001b[39mreturn\u001b[39;00m get_resampler_for_grouping(\u001b[39mself\u001b[39;49m, rule, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/resample.py:1558\u001b[0m, in \u001b[0;36mget_resampler_for_grouping\u001b[0;34m(groupby, rule, how, fill_method, limit, kind, on, **kwargs)\u001b[0m\n\u001b[1;32m   1556\u001b[0m \u001b[39m# .resample uses 'on' similar to how .groupby uses 'key'\u001b[39;00m\n\u001b[1;32m   1557\u001b[0m tg \u001b[39m=\u001b[39m TimeGrouper(freq\u001b[39m=\u001b[39mrule, key\u001b[39m=\u001b[39mon, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m-> 1558\u001b[0m resampler \u001b[39m=\u001b[39m tg\u001b[39m.\u001b[39;49m_get_resampler(groupby\u001b[39m.\u001b[39;49mobj, kind\u001b[39m=\u001b[39;49mkind)\n\u001b[1;32m   1559\u001b[0m \u001b[39mreturn\u001b[39;00m resampler\u001b[39m.\u001b[39m_get_resampler_for_grouping(groupby\u001b[39m=\u001b[39mgroupby, key\u001b[39m=\u001b[39mtg\u001b[39m.\u001b[39mkey)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/resample.py:1709\u001b[0m, in \u001b[0;36mTimeGrouper._get_resampler\u001b[0;34m(self, obj, kind)\u001b[0m\n\u001b[1;32m   1690\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_resampler\u001b[39m(\u001b[39mself\u001b[39m, obj, kind\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m   1691\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1692\u001b[0m \u001b[39m    Return my resampler or raise if we have an invalid axis.\u001b[39;00m\n\u001b[1;32m   1693\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1707\u001b[0m \n\u001b[1;32m   1708\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1709\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_set_grouper(obj)\n\u001b[1;32m   1711\u001b[0m     ax \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39max\n\u001b[1;32m   1712\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(ax, DatetimeIndex):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/groupby/grouper.py:406\u001b[0m, in \u001b[0;36mGrouper._set_grouper\u001b[0;34m(self, obj, sort)\u001b[0m\n\u001b[1;32m    402\u001b[0m \u001b[39m# possibly sort\u001b[39;00m\n\u001b[1;32m    403\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msort \u001b[39mor\u001b[39;00m sort) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m ax\u001b[39m.\u001b[39mis_monotonic_increasing:\n\u001b[1;32m    404\u001b[0m     \u001b[39m# use stable sort to support first, last, nth\u001b[39;00m\n\u001b[1;32m    405\u001b[0m     \u001b[39m# TODO: why does putting na_position=\"first\" fix datetimelike cases?\u001b[39;00m\n\u001b[0;32m--> 406\u001b[0m     indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindexer \u001b[39m=\u001b[39m ax\u001b[39m.\u001b[39;49marray\u001b[39m.\u001b[39margsort(\n\u001b[1;32m    407\u001b[0m         kind\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmergesort\u001b[39m\u001b[39m\"\u001b[39m, na_position\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mfirst\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    408\u001b[0m     )\n\u001b[1;32m    409\u001b[0m     ax \u001b[39m=\u001b[39m ax\u001b[39m.\u001b[39mtake(indexer)\n\u001b[1;32m    410\u001b[0m     obj \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39mtake(indexer, axis\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxis)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/indexes/multi.py:762\u001b[0m, in \u001b[0;36mMultiIndex.array\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    752\u001b[0m \u001b[39m@property\u001b[39m\n\u001b[1;32m    753\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39marray\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    754\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    755\u001b[0m \u001b[39m    Raises a ValueError for `MultiIndex` because there's no single\u001b[39;00m\n\u001b[1;32m    756\u001b[0m \u001b[39m    array backing a MultiIndex.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    760\u001b[0m \u001b[39m    ValueError\u001b[39;00m\n\u001b[1;32m    761\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 762\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    763\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mMultiIndex has no single backing array. Use \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    764\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39mMultiIndex.to_numpy()\u001b[39m\u001b[39m'\u001b[39m\u001b[39m to get a NumPy array of tuples.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    765\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: MultiIndex has no single backing array. Use 'MultiIndex.to_numpy()' to get a NumPy array of tuples."
     ]
    }
   ],
   "source": [
    "df_demand = df_demand.set_index(['pickup_hex', 'timestamp'])\n",
    "df_resampled = df_demand.groupby('pickup_hex').resample('H').sum()\n",
    "all_hexagons = df_demand.index.get_level_values('pickup_hex').unique()\n",
    "all_hours = pd.date_range(start=df_demand.index.get_level_values('timestamp').min().floor('D'),\n",
    "                          end=df_demand.index.get_level_values('timestamp').max().ceil('D'),\n",
    "                          freq='H')\n",
    "index = pd.MultiIndex.from_product([all_hexagons, all_hours], names=['pickup_hex', 'timestamp'])\n",
    "df_all_combinations = pd.DataFrame(index=index).reset_index()\n",
    "df_merged = pd.merge(df_all_combinations, df_resampled, on=['pickup_hex', 'timestamp'], how='left')\n",
    "df_merged = df_merged.fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "203506"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_demand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trip_start_hour</th>\n",
       "      <th>trip_start_month</th>\n",
       "      <th>trip_start_day</th>\n",
       "      <th>pickup_hex</th>\n",
       "      <th>demand</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-12-20 09:00:00</th>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "      <td>20</td>\n",
       "      <td>8a2664c1e4effff</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-11 16:00:00</th>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>8a2664c1e8cffff</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-03-20 12:00:00</th>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "      <td>8a2664c1e32ffff</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-04-29 21:00:00</th>\n",
       "      <td>21</td>\n",
       "      <td>4</td>\n",
       "      <td>29</td>\n",
       "      <td>8a2664c1e0effff</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-04-18 17:00:00</th>\n",
       "      <td>17</td>\n",
       "      <td>4</td>\n",
       "      <td>18</td>\n",
       "      <td>8a2664c1acd7fff</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-03 00:00:00</th>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>8a2664ca1a0ffff</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-06 15:00:00</th>\n",
       "      <td>15</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>8a2664d9d76ffff</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-07-06 21:00:00</th>\n",
       "      <td>21</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>8a2664d88457fff</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-15 23:00:00</th>\n",
       "      <td>23</td>\n",
       "      <td>9</td>\n",
       "      <td>15</td>\n",
       "      <td>8a2664cab057fff</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-27 08:00:00</th>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>27</td>\n",
       "      <td>8a2664cb5b6ffff</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1095 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     trip_start_hour  trip_start_month  trip_start_day  \\\n",
       "timestamp                                                                \n",
       "2017-12-20 09:00:00                9                12              20   \n",
       "2017-01-11 16:00:00               16                 1              11   \n",
       "2017-03-20 12:00:00               12                 3              20   \n",
       "2017-04-29 21:00:00               21                 4              29   \n",
       "2017-04-18 17:00:00               17                 4              18   \n",
       "...                              ...               ...             ...   \n",
       "2017-12-03 00:00:00                0                12               3   \n",
       "2017-09-06 15:00:00               15                 9               6   \n",
       "2017-07-06 21:00:00               21                 7               6   \n",
       "2017-09-15 23:00:00               23                 9              15   \n",
       "2017-09-27 08:00:00                8                 9              27   \n",
       "\n",
       "                          pickup_hex  demand  \n",
       "timestamp                                     \n",
       "2017-12-20 09:00:00  8a2664c1e4effff       5  \n",
       "2017-01-11 16:00:00  8a2664c1e8cffff       5  \n",
       "2017-03-20 12:00:00  8a2664c1e32ffff       8  \n",
       "2017-04-29 21:00:00  8a2664c1e0effff      21  \n",
       "2017-04-18 17:00:00  8a2664c1acd7fff      16  \n",
       "...                              ...     ...  \n",
       "2017-12-03 00:00:00  8a2664ca1a0ffff       1  \n",
       "2017-09-06 15:00:00  8a2664d9d76ffff       1  \n",
       "2017-07-06 21:00:00  8a2664d88457fff       1  \n",
       "2017-09-15 23:00:00  8a2664cab057fff       1  \n",
       "2017-09-27 08:00:00  8a2664cb5b6ffff       1  \n",
       "\n",
       "[1095 rows x 5 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_demand.groupby('pickup_hex').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1242/860228888.py:2: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  df_demand_resampled = df_demand.groupby('pickup_hex').resample('D').sum()\n"
     ]
    }
   ],
   "source": [
    "df_demand=df_demand.set_index('timestamp')\n",
    "df_demand_resampled = df_demand.groupby('pickup_hex').resample('D').sum()\n",
    "df_demand_resampled = df_demand_resampled.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true\n",
      "true\n",
      "true\n",
      "true\n",
      "true\n",
      "true\n",
      "true\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true\n",
      "true\n",
      "true\n",
      "true\n",
      "true\n",
      "true\n",
      "true\n",
      "true\n",
      "true\n",
      "true\n",
      "true\n",
      "true\n",
      "true\n",
      "true\n",
      "true\n",
      "true\n",
      "true\n",
      "true\n",
      "true\n",
      "true\n",
      "true\n",
      "true\n",
      "true\n",
      "true\n",
      "true\n",
      "true\n",
      "true\n",
      "true\n",
      "true\n",
      "true\n",
      "true\n",
      "true\n",
      "true\n",
      "true\n",
      "true\n",
      "true\n",
      "true\n",
      "true\n",
      "true\n",
      "true\n",
      "true\n",
      "true\n",
      "true\n"
     ]
    }
   ],
   "source": [
    "hexagons = df_demand_resampled[\"pickup_hex\"].unique()\n",
    "\n",
    "for hexagon in hexagons:\n",
    "    if len(df_demand_resampled[df_demand_resampled[\"pickup_hex\"] == str(hexagon)]['timestamp'].dt.date.unique()) == 365:\n",
    "        print(\"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "210\n"
     ]
    }
   ],
   "source": [
    "hexagons = df_demand_resampled[\"pickup_hex\"].unique()\n",
    "hexagons_with_empty_days = []\n",
    "for hexagon in hexagons:\n",
    "    df_one_hexagon = df_demand_resampled[df_demand_resampled[\"pickup_hex\"] == str(hexagon)]\n",
    "    df_one_hexagon.reset_index(inplace=True)\n",
    "    if len(df_one_hexagon['timestamp'].dt.date.unique()) != 365:\n",
    "        hexagons_with_empty_days.append(str(hexagon))\n",
    "    \n",
    "print(len(hexagons_with_empty_days))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1242/40368608.py:3: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  df_demand_resampled = df_demand.groupby('pickup_hex').resample('H').sum()\n"
     ]
    }
   ],
   "source": [
    "# insert 0 values for hours without demand\n",
    "df_demand=df_demand.set_index('timestamp')\n",
    "\n",
    "df_demand_resampled = df_demand.groupby('pickup_hex').resample('H').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert features 1, 2 and 24 hours previous demand\n",
    "df_demand_resampled['demand_h-1'] = df_demand_resampled.sort_values('timestamp').groupby('pickup_hex')['demand'].shift(1)\n",
    "df_demand_resampled['demand_h-2'] = df_demand_resampled.sort_values('timestamp').groupby('pickup_hex')['demand'].shift(2)\n",
    "df_demand_resampled['demand_h-24'] = df_demand_resampled.sort_values('timestamp').groupby('pickup_hex')['demand'].shift(24)\n",
    "df_demand_resampled.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_demand(df, hour_shift, hour, month, day):\n",
    "    winter = [12, 1, 2]\n",
    "    spring = [3,4,5]\n",
    "    summer = [6,7,8]\n",
    "    autumn = [9,10,11]\n",
    "\n",
    "    months = []\n",
    "    if month in winter:\n",
    "        months = winter\n",
    "    elif month in spring:\n",
    "        months = spring\n",
    "    elif month in summer:\n",
    "        months = summer\n",
    "    else:\n",
    "        months = autumn\n",
    "    \n",
    "    return df_demand.filter(df_demand['trip_start_hour']((df_demand['trip_start_month'] == months[0]) | (df_demand['trip_start_month'] == months[1]) | (df_demand['trip_start_month'] == months[2])))['demand'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weather features\n",
    "In the descriptive analysis, particularly the analysis of temporal demand patterns, we found that the temperature and demand curves follow similar directions. Therefore, we construct features based on temperature to enable models that capture this relationship."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Include weather data\n",
    "First, we have to include the weather data into the dataframe. For this we just need to merge the two datasets, as both are already in hourly frequency. The weather data propose data for minute 53 of an hour. Therefore, we round up to the nearest hour for each row. We suppose that the weather changes in seven minutes can be disregarded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "df_weather = pd.read_csv('data/weather_data_final.csv')\n",
    "df_weather['date_time'] = pd.to_datetime(df_weather['date_time'])\n",
    "df_weather['date_time'] = df_weather['date_time'].dt.ceil('H')\n",
    "df_weather.rename(columns={'date_time': 'timestamp'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>temp</th>\n",
       "      <th>dew_point</th>\n",
       "      <th>humidity</th>\n",
       "      <th>wind_speed</th>\n",
       "      <th>wind_gust</th>\n",
       "      <th>pressure</th>\n",
       "      <th>precip</th>\n",
       "      <th>condition</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>00:53</td>\n",
       "      <td>33 °F</td>\n",
       "      <td>24 °F</td>\n",
       "      <td>70 °%</td>\n",
       "      <td>8 °mph</td>\n",
       "      <td>0 °mph</td>\n",
       "      <td>29.45 °in</td>\n",
       "      <td>0.0 °in</td>\n",
       "      <td>Partly Cloudy</td>\n",
       "      <td>2017-01-01 01:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date   time   temp dew_point humidity wind_speed wind_gust  \\\n",
       "0  2017-01-01  00:53  33 °F     24 °F    70 °%     8 °mph    0 °mph   \n",
       "\n",
       "    pressure   precip      condition           timestamp  \n",
       "0  29.45 °in  0.0 °in  Partly Cloudy 2017-01-01 01:00:00  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_weather.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_demand_merged = df_demand_resampled.merge(df_weather, on='timestamp', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temperature features\n",
    "In addition to the current temperature, we are add the temperature from 1, 2, and 3 hours prior to the time of taxi demand. We suggest that past temperature conditions could potentially impact the decision to hire a taxi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_demand_merged['temp_h-1'] = df_demand_merged.sort_values('timestamp').groupby('pickup_hex')['temp'].shift(1)\n",
    "df_demand_merged['temp_h-2'] = df_demand_merged.sort_values('timestamp').groupby('pickup_hex')['temp'].shift(2)\n",
    "df_demand_merged['temp_h-3'] = df_demand_merged.sort_values('timestamp').groupby('pickup_hex')['temp'].shift(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precipitation\n",
    "We hypothesize that precipitation has a significant impact on demand. Therefore, we construct features that describe whether it has rained in the last 1-3 hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_demand_merged['precip_h-1'] = df_demand_merged.sort_values('timestamp').groupby('pickup_hex')['precip'].shift(1)\n",
    "df_demand_merged['precip_h-2'] = df_demand_merged.sort_values('timestamp').groupby('pickup_hex')['precip'].shift(2)\n",
    "df_demand_merged['precip_h-3'] = df_demand_merged.sort_values('timestamp').groupby('pickup_hex')['precip'].shift(3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
