{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import local_helpers as lh\n",
    "import vaex\n",
    "import h3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "import shapely\n",
    "from shapely import wkt\n",
    "import category_encoders as ce\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "\n",
    "### Previous demand as input\n",
    "\n",
    "As we have given time series data, it is a common approach to use the demand of previous hours (or days etc.) as an input for the prediction. The assumption we hereby make is that the factors that influence the demand have not changed dramatically within the used time frames. We have decided to construct the following features from previous demand:\n",
    "\n",
    "* 2 hour: The asssumption is that the demand should not change dramatically between three hours.\n",
    "* 24 hours: The asssumption is that the current demand should be comparable to the demand exactly one day ago, as factors such as season, time of the day are the same.\n",
    "* Average demand of the past week at the same day time: This feature is the average of all 7 demand observations of the past week at same time of the day. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This functions loads the dataset either with hexagons or census tract\n",
    "def load_dataset_to_pandas(resolution=10, location_type_hexagon = True):\n",
    "    df = vaex.open('./data/trips_prepared.hdf5')\n",
    "    df.head()\n",
    "\n",
    "    df[\"trip_start_day\"] = df.trip_start_timestamp.dt.day\n",
    "    df[\"trip_start_month\"] = df.trip_start_timestamp.dt.month\n",
    "    df[\"trip_start_hour\"] = df.trip_start_timestamp.dt.hour\n",
    "    df[\"trip_start_minute\"] = df.trip_start_timestamp.dt.minute\n",
    "\n",
    "    if location_type_hexagon:\n",
    "        \n",
    "        \n",
    "    \n",
    "        RESOLUTION = resolution\n",
    "        def geo_to_h3(row1, row2):\n",
    "            return h3.geo_to_h3(row1,row2, RESOLUTION)\n",
    "\n",
    "        # Step 1: For each pickup and drop-off calculate the correct hexagon in the resolution\n",
    "        df['pickup_loc'] = df.apply(geo_to_h3, [df['pickup_centroid_latitude'], df['pickup_centroid_longitude']])\n",
    "    else:\n",
    "        df.rename('pickup_census_tract', 'pickup_loc')\n",
    "\n",
    "    ### Group by hour\n",
    "    df_demand = df.groupby(['trip_start_hour', 'trip_start_month', 'trip_start_day', 'pickup_loc']).agg({'demand': 'count'})\n",
    "    # Add timestamp as preparation for resampling\n",
    "    df_demand['timestamp'] = pd.to_datetime({'year': 2017, 'month': df_demand['trip_start_month'].to_numpy(), 'day': df_demand['trip_start_day'].to_numpy(), 'hour': df_demand['trip_start_hour'].to_numpy()}).to_numpy()\n",
    "\n",
    "    # convert to pandas df\n",
    "    df_demand = df_demand.to_pandas_df()\n",
    "    return df_demand\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_to_hourly(df):\n",
    "    ### Creation of dummy df which contains hourly data dummy data over an entire year per hexagon\n",
    "    # Create a DateTimeIndex with hourly intervals for the year 2017\n",
    "    start_date = '2017-01-01 00:00:00'\n",
    "    end_date = '2017-12-31 23:00:00'\n",
    "    hourly_range = pd.date_range(start=start_date, end=end_date, freq='H')\n",
    "    num_entries_per_year = len(hourly_range)\n",
    "    hour_range = np.tile(hourly_range.hour,len(np.unique(df.pickup_loc)))\n",
    "    month_range = np.tile(hourly_range.month,len(np.unique(df.pickup_loc)))\n",
    "    day_range = np.tile(hourly_range.day,len(np.unique(df.pickup_loc)))\n",
    "    hourly_range = np.tile(hourly_range,len(np.unique(df.pickup_loc)))\n",
    "\n",
    "    # -1 values will indacte that these rows were artificially generated later on\n",
    "    data = {\n",
    "        'trip_start_hour': hour_range,\n",
    "        'trip_start_month': month_range,\n",
    "        'trip_start_day': day_range,\n",
    "        'pickup_loc': np.repeat(np.unique(df.pickup_loc), num_entries_per_year),\n",
    "        'demand': 0,\n",
    "    }\n",
    "\n",
    "    df_hourly = pd.DataFrame(data, index=hourly_range)\n",
    "    df_hourly= df_hourly.set_index([df_hourly.index, 'pickup_loc'])\n",
    "\n",
    "    # introduce multiindex for filling up the df with hourly index later on\n",
    "    df=df.set_index(['timestamp', 'pickup_loc'])\n",
    "\n",
    "    # insert df \n",
    "    df_hourly.update(df)\n",
    "\n",
    "    # clear up multi-index\n",
    "    df_hourly=df_hourly.reset_index()\n",
    "    df_hourly.columns = ['timestamp','pickup_loc','trip_start_hour','trip_start_month','trip_start_day','demand']\n",
    "    return df_hourly\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target_time_bucket_demand(df):\n",
    "    \n",
    "    demand_cols = [f'demand_n-{num}' for num in list(range(0,25))]\n",
    "    \n",
    "    for shift in list(range(0,25)):\n",
    "        df[f'demand_n-{shift}'] =  df.sort_values('timestamp').groupby('pickup_loc')['demand'].shift((-1)*shift)\n",
    "        \n",
    "    two_hours = ['demand_n-0','demand_n-1']\n",
    "    six_hours = [f'demand_n-{num}' for num in list(range(0,6))]\n",
    "    twentyfour_hours = [f'demand_n-{num}' for num in list(range(0,24))]\n",
    "    \n",
    "    df['demand_target_bucket_1'] = df['demand_n-0']\n",
    "    df['demand_target_bucket_2'] = df[two_hours].sum(axis=1)\n",
    "    df['demand_target_bucket_6'] = df[six_hours].sum(axis=1)\n",
    "    df['demand_target_bucket_24'] = df[twentyfour_hours].sum(axis=1)\n",
    "    df = df.drop(labels=demand_cols, axis=1)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_past_time_bucket_demand(df):\n",
    "    \n",
    "    demand_cols = [f'demand_p-{num}' for num in list(range(0,24))]\n",
    "    \n",
    "    for shift in list(range(0,25)):\n",
    "        df[f'demand_p-{shift}'] =  df.sort_values('timestamp').groupby('pickup_loc')['demand'].shift(shift)\n",
    "        \n",
    "    two_hours = ['demand_p-0', 'demand_p-1']\n",
    "    six_hours = [f'demand_p-{num}' for num in list(range((24-5),25))]\n",
    "    twentyfour_hours = [f'demand_p-{num}' for num in list(range(1,25))]\n",
    "    \n",
    "    df['demand_bucket_2'] = df[two_hours].sum(axis=1)\n",
    "    df['demand_bucket_6'] = df[six_hours].sum(axis=1)\n",
    "    df['demand_bucket_24'] = df[twentyfour_hours].sum(axis=1)\n",
    "    df = df.drop(labels=demand_cols, axis=1)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_past_single_hour_demand(df):\n",
    "    # insert features 1, 2 and 24 hours previous demand\n",
    "    df['demand_h-1'] = df.sort_values('timestamp').groupby('pickup_loc')['demand'].shift(1)\n",
    "    df['demand_h-2'] = df.sort_values('timestamp').groupby('pickup_loc')['demand'].shift(2)\n",
    "    df['demand_h-24'] = df.sort_values('timestamp').groupby('pickup_loc')['demand'].shift(24)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weather features\n",
    "In the descriptive analysis, particularly the analysis of temporal demand patterns, we found that the temperature and demand curves follow similar directions. Therefore, we construct features based on temperature to enable models that capture this relationship."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Include weather data\n",
    "First, we have to include the weather data into the dataframe. For this we just need to merge the two datasets, as both are already in hourly frequency. The weather data propose data for minute 53 of an hour. Therefore, we round up to the nearest hour for each row. We suppose that the weather changes in seven minutes can be disregarded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weather_categories(weather):\n",
    "    if \"Fair\" in weather:\n",
    "        return \"Clear/Sunny\"\n",
    "    elif \"Cloudy\" in weather:\n",
    "        return \"Cloudy\"\n",
    "    elif \"Rain\" in weather or \"Drizzle\" in weather or \"T-Storm\" in weather or \"Thunder\" in weather:\n",
    "        return \"Rain/Storms\"\n",
    "    elif \"Snow\" in weather or \"Wintry\" in weather:\n",
    "        return \"Snow/Winter Conditions\"\n",
    "    elif weather in [\"Fog\", \"Haze\", \"Smoke\", \"Widespread Dust / Windy\"]:\n",
    "        return \"Other\"\n",
    "    else:\n",
    "        return \"Miscellaneous\"\n",
    "    \n",
    "def merge_weather(df):\n",
    "    # read and merge weather data\n",
    "    df_weather = pd.read_csv('data/weather_data_final.csv')\n",
    "    df_weather['date_time'] = pd.to_datetime(df_weather['date_time'])\n",
    "    df_weather['date_time'] = df_weather['date_time'].dt.ceil('H')\n",
    "    df_weather.rename(columns={'date_time': 'timestamp'}, inplace=True)\n",
    "\n",
    "    # cast data types\n",
    "    df_weather['temp'] = df_weather['temp'].str.replace('\\xa0Â°F', '').astype(float).fillna(np.nan)\n",
    "    df_weather['dew_point'] = df_weather['dew_point'].str.replace('\\xa0Â°F', '').astype(float).fillna(np.nan)\n",
    "    df_weather['humidity'] = df_weather['humidity'].str.replace('\\xa0Â°%', '').astype(float).fillna(np.nan)\n",
    "    df_weather['wind_speed'] = df_weather['wind_speed'].str.replace('\\xa0Â°mph', '').astype(float).fillna(np.nan)\n",
    "    df_weather['wind_gust'] = df_weather['wind_gust'].str.replace('\\xa0Â°mph', '').astype(float).fillna(np.nan)\n",
    "    df_weather['pressure'] = df_weather['pressure'].str.replace('\\xa0Â°in', '').astype(float).fillna(np.nan)\n",
    "    df_weather['precip'] = df_weather['precip'].str.replace('\\xa0Â°in', '').astype(float).fillna(np.nan)\n",
    "    df_weather = df_weather.drop(['time','date', 'dew_point'], axis=1)\n",
    "    df_weather['condition'] = df_weather[\"condition\"].apply(weather_categories)\n",
    "    df_weather = pd.get_dummies(df_weather, columns=['condition'])\n",
    "\n",
    "\n",
    "    df = df.merge(df_weather, on='timestamp', how='left')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temperature features\n",
    "In addition to the current temperature, we are add the temperature from 1, 2, and 3 hours prior to the time of taxi demand. We suggest that past temperature conditions could potentially impact the decision to hire a taxi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_past_temperature(df):\n",
    "    df['temp_h-1'] = df.sort_values('timestamp').groupby('pickup_loc')['temp'].shift(1)\n",
    "    df['temp_h-2'] = df.sort_values('timestamp').groupby('pickup_loc')['temp'].shift(2)\n",
    "    df['temp_h-3'] = df.sort_values('timestamp').groupby('pickup_loc')['temp'].shift(3)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precipitation\n",
    "We hypothesize that precipitation has a significant impact on demand. Therefore, we construct features that describe whether it has rained in the last 1-3 hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_past_precip(df):\n",
    "    df['precip_h-1'] = df.sort_values('timestamp').groupby('pickup_loc')['precip'].shift(1)\n",
    "    df['precip_h-2'] = df.sort_values('timestamp').groupby('pickup_loc')['precip'].shift(2)\n",
    "    df['precip_h-3'] = df.sort_values('timestamp').groupby('pickup_loc')['precip'].shift(3)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Season\n",
    "We suggest that demand changes over seasons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_to_season(date):\n",
    "    if pd.Timestamp(2017, 12, 22) <= date or date < pd.Timestamp(2017, 3, 20):\n",
    "        return 'winter'\n",
    "    elif pd.Timestamp(2017, 3, 20) <= date < pd.Timestamp(2017, 6, 21):\n",
    "        return 'spring'\n",
    "    elif pd.Timestamp(2017, 6, 21) <= date < pd.Timestamp(2017, 9, 23):\n",
    "        return 'summer'\n",
    "    else:\n",
    "        return 'autumn'\n",
    "\n",
    "def get_season(df):\n",
    "    # Vectorize the date_to_season function\n",
    "    date_to_season_vec = np.vectorize(date_to_season)\n",
    "    \n",
    "    # Apply the vectorized function to the 'timestamp' column\n",
    "    df['season'] = date_to_season_vec(df['timestamp'])\n",
    "    \n",
    "    # Create dummies\n",
    "    df = pd.get_dummies(df, columns=['season'])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weekend feature\n",
    "In the descriptive analysis we have seen that demand changes depending on weekend or not. Hence we engineer a feature \"weekend\" which is 1 for all rides on saturday & sunday and zero for all other days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weekend(df):\n",
    "    df['weekend'] = (df['timestamp'].dt.weekday >= 5).astype(int)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Daytime features\n",
    "In addition, descriptive analysis has shown that the time of day, i.e., whether it is night, morning, afternoon, or evening, plays an important role in determining demand. Therefore, we developed four characteristics, each indicating whether a trip occurs during the following times.\n",
    "* Morning: 6 a.m. - 12 p.m.\n",
    "* Afternoon: 12 noon - 6 p.m.\n",
    "* Evening: 6 p.m. - 11 p.m.\n",
    "* Night: 12 a.m. - 6 a.m."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pnt_day_with_pnt_week(dt):\n",
    "    dt = pd.to_datetime(dt)  \n",
    "    time_of_week = \"week\" if dt.weekday() < 5 else \"weekend\"\n",
    "    hour = dt.hour\n",
    "\n",
    "    if 6 <= hour < 12:\n",
    "        return \"morning_\" + time_of_week\n",
    "    elif 12 <= hour < 18:\n",
    "        return \"afternoon_\" + time_of_week\n",
    "    elif 18 <= hour < 23:\n",
    "        return \"evening_\" + time_of_week\n",
    "    else:\n",
    "        return \"night_\" + time_of_week\n",
    "\n",
    "def get_daytime(df):\n",
    "    vfunc = np.vectorize(get_pnt_day_with_pnt_week)\n",
    "    df['daytime'] = vfunc(df['timestamp'])\n",
    "    dummies = pd.get_dummies(df['daytime']).astype(int)\n",
    "    df = pd.concat([df.drop(['daytime'], axis=1), dummies], axis=1)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Event Features\n",
    "Events such as public holidays might influence the demand. In order to capture these patterns we introduce the public holiday feature\n",
    "\n",
    "There were several public holidays in Boston:\n",
    "* Martin Luther King Day: Monday, January 16, 2017\n",
    "* Lincoln's Birthday: Monday, February 13, 2017\n",
    "* Washington's Birthday (President's Day): Monday, February 20, 2017\n",
    "* Memorial Day: Monday, May 29, 2017\n",
    "* Independence Day: Tuesday, July 04, 2017\n",
    "* Labor Day: Monday, September 04, 2017\n",
    "* Columbus Day: Monday, October 09, 2017\n",
    "* Veterans' Day: Friday, November 10, 2017\n",
    "* Thanksgiving Day: Thursday, November 23, 2017\n",
    "* Thanksgiving Day: Friday, November 24, 2017\n",
    "* Christmas Day: Monday, December 25, 2017\n",
    "\n",
    "These events might have influenced the demand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "holiday_dates = [\n",
    "    datetime(2017, 1, 2),   # New Year's Day\n",
    "    datetime(2017, 1, 16),  # Martin Luther King Day\n",
    "    datetime(2017, 2, 13),  # Lincoln's Birthday\n",
    "    datetime(2017, 2, 20),  # Washington's Birthday (President's Day)\n",
    "    datetime(2017, 5, 29),  # Memorial Day\n",
    "    datetime(2017, 7, 4),   # Independence Day\n",
    "    datetime(2017, 9, 4),   # Labor Day\n",
    "    datetime(2017, 10, 9),  # Columbus Day\n",
    "    datetime(2017, 11, 10), # Veterans' Day\n",
    "    datetime(2017, 11, 23), # Thanksgiving Day\n",
    "    datetime(2017, 11, 24), # Day after Thanksgiving\n",
    "    datetime(2017, 12, 25), # Christmas Day\n",
    "]\n",
    "\n",
    "def get_holiday_dates(df):\n",
    "    df['is_holiday'] = df['timestamp'].isin(holiday_dates).astype(int)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neighbor demand feature\n",
    "We expect a high correlation between the demand of one hexagon and the demand in the surrounding hexagons. With this feature we can observe demand patterns in a greater radius than only in the observed location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_dataset_to_pandas(location_type_hexagon=False) \n",
    "df = resample_to_hourly(df)\n",
    "df = get_past_single_hour_demand(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_past_time_bucket_demand(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>pickup_loc</th>\n",
       "      <th>demand</th>\n",
       "      <th>demand_h-1</th>\n",
       "      <th>demand_h-2</th>\n",
       "      <th>demand_h-24</th>\n",
       "      <th>demand_p-24</th>\n",
       "      <th>demand_bucket_2</th>\n",
       "      <th>demand_bucket_6</th>\n",
       "      <th>demand_bucket_24</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-01-01 00:00:00</td>\n",
       "      <td>1.703101e+10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-01-01 01:00:00</td>\n",
       "      <td>1.703101e+10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-01-01 02:00:00</td>\n",
       "      <td>1.703101e+10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-01-01 03:00:00</td>\n",
       "      <td>1.703101e+10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-01-01 04:00:00</td>\n",
       "      <td>1.703101e+10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3127315</th>\n",
       "      <td>2017-12-31 19:00:00</td>\n",
       "      <td>1.703198e+10</td>\n",
       "      <td>4.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>204.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3127316</th>\n",
       "      <td>2017-12-31 20:00:00</td>\n",
       "      <td>1.703198e+10</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>191.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3127317</th>\n",
       "      <td>2017-12-31 21:00:00</td>\n",
       "      <td>1.703198e+10</td>\n",
       "      <td>10.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>184.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3127318</th>\n",
       "      <td>2017-12-31 22:00:00</td>\n",
       "      <td>1.703198e+10</td>\n",
       "      <td>13.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>174.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3127319</th>\n",
       "      <td>2017-12-31 23:00:00</td>\n",
       "      <td>1.703198e+10</td>\n",
       "      <td>2.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>176.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3127320 rows Ã 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  timestamp    pickup_loc  demand  demand_h-1  demand_h-2  \\\n",
       "0       2017-01-01 00:00:00  1.703101e+10     0.0         NaN         NaN   \n",
       "1       2017-01-01 01:00:00  1.703101e+10     0.0         0.0         NaN   \n",
       "2       2017-01-01 02:00:00  1.703101e+10     0.0         0.0         0.0   \n",
       "3       2017-01-01 03:00:00  1.703101e+10     0.0         0.0         0.0   \n",
       "4       2017-01-01 04:00:00  1.703101e+10     0.0         0.0         0.0   \n",
       "...                     ...           ...     ...         ...         ...   \n",
       "3127315 2017-12-31 19:00:00  1.703198e+10     4.0        15.0        14.0   \n",
       "3127316 2017-12-31 20:00:00  1.703198e+10     9.0         4.0        15.0   \n",
       "3127317 2017-12-31 21:00:00  1.703198e+10    10.0         9.0         4.0   \n",
       "3127318 2017-12-31 22:00:00  1.703198e+10    13.0        10.0         9.0   \n",
       "3127319 2017-12-31 23:00:00  1.703198e+10     2.0        13.0        10.0   \n",
       "\n",
       "         demand_h-24  demand_p-24  demand_bucket_2  demand_bucket_6  \\\n",
       "0                NaN          NaN              0.0              0.0   \n",
       "1                NaN          NaN              0.0              0.0   \n",
       "2                NaN          NaN              0.0              0.0   \n",
       "3                NaN          NaN              0.0              0.0   \n",
       "4                NaN          NaN              0.0              0.0   \n",
       "...              ...          ...              ...              ...   \n",
       "3127315         17.0         17.0             19.0             97.0   \n",
       "3127316         16.0         16.0             13.0             88.0   \n",
       "3127317         20.0         20.0             19.0             76.0   \n",
       "3127318         11.0         11.0             23.0             57.0   \n",
       "3127319         20.0         20.0             15.0             46.0   \n",
       "\n",
       "         demand_bucket_24  \n",
       "0                     0.0  \n",
       "1                     0.0  \n",
       "2                     0.0  \n",
       "3                     0.0  \n",
       "4                     0.0  \n",
       "...                   ...  \n",
       "3127315             204.0  \n",
       "3127316             191.0  \n",
       "3127317             184.0  \n",
       "3127318             174.0  \n",
       "3127319             176.0  \n",
       "\n",
       "[3127320 rows x 10 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop(columns=[\"trip_start_hour\",\"trip_start_month\",\"trip_start_day\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load data from CSV and preprocess it\n",
    "def preprocess_census_data():\n",
    "    df_census_tracts = pd.read_csv(\"data/chicago_census_tracts.csv\")\n",
    "    df_census_tracts['geometry'] = df_census_tracts['the_geom'].apply(wkt.loads)\n",
    "    df_census_tracts = df_census_tracts.rename(columns={'geometry': 'geometry'})\n",
    "\n",
    "    gdf = gpd.GeoDataFrame(df_census_tracts, geometry='geometry')\n",
    "    gdf = gdf.set_crs(epsg=4326, allow_override=True)\n",
    "    gdf = gdf.to_crs(epsg=3857)\n",
    "    return gdf\n",
    "\n",
    "# Function to get neighbors using H3\n",
    "def get_h3_neighbors(df):\n",
    "    pickup_locs = df[\"pickup_loc\"].unique()\n",
    "    return {loc: h3.k_ring(loc, k=1) for loc in pickup_locs}\n",
    "\n",
    "# Function to get neighbors from Census data\n",
    "def get_census_neighbors(gdf, buffer_distance=500):\n",
    "    gdf_buffered = gdf.copy()\n",
    "    gdf_buffered['geometry'] = gdf.buffer(buffer_distance)\n",
    "    gdf['geometry'] = gdf_buffered.geometry  # Replace original geometry with buffered one\n",
    "    census_tract_neighbors = gpd.sjoin(gdf, gdf, how='left', predicate='intersects')\n",
    "    census_tract_neighbors = census_tract_neighbors[census_tract_neighbors.index != census_tract_neighbors.index_right]\n",
    "    return census_tract_neighbors.groupby('GEOID10_left')['GEOID10_right'].apply(list).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get dataframe with mean demand value of each hexagon/census tract\n",
    "def get_neighbor_demand(df):\n",
    "    locs = df[\"pickup_loc\"].unique\n",
    "    if h3.h3_is_valid(df[\"pickup_loc\"].iloc[0]):\n",
    "        neighbors_dict = get_h3_neighbors(df)\n",
    "    else:\n",
    "        gdf = preprocess_census_data()\n",
    "        neighbors_dict = get_census_neighbors(gdf)\n",
    "\n",
    "    df['neighbors'] = df['pickup_loc'].map(neighbors_dict)\n",
    "    df = df.drop(columns=[\"trip_start_hour\", \"trip_start_month\", \"trip_start_day\"])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>pickup_loc</th>\n",
       "      <th>demand</th>\n",
       "      <th>demand_h-1</th>\n",
       "      <th>demand_h-2</th>\n",
       "      <th>demand_h-24</th>\n",
       "      <th>demand_p-24</th>\n",
       "      <th>demand_bucket_2</th>\n",
       "      <th>demand_bucket_6</th>\n",
       "      <th>demand_bucket_24</th>\n",
       "      <th>neighbors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-01-01 00:00:00</td>\n",
       "      <td>1.703101e+10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[17031020100, 17031010400, 17031010501, 170310...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-01-01 01:00:00</td>\n",
       "      <td>1.703101e+10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[17031020100, 17031010400, 17031010501, 170310...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-01-01 02:00:00</td>\n",
       "      <td>1.703101e+10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[17031020100, 17031010400, 17031010501, 170310...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-01-01 03:00:00</td>\n",
       "      <td>1.703101e+10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[17031020100, 17031010400, 17031010501, 170310...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-01-01 04:00:00</td>\n",
       "      <td>1.703101e+10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[17031020100, 17031010400, 17031010501, 170310...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3127315</th>\n",
       "      <td>2017-12-31 19:00:00</td>\n",
       "      <td>1.703198e+10</td>\n",
       "      <td>4.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>204.0</td>\n",
       "      <td>[17031640800, 17031650301, 17031640100, 170316...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3127316</th>\n",
       "      <td>2017-12-31 20:00:00</td>\n",
       "      <td>1.703198e+10</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>191.0</td>\n",
       "      <td>[17031640800, 17031650301, 17031640100, 170316...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3127317</th>\n",
       "      <td>2017-12-31 21:00:00</td>\n",
       "      <td>1.703198e+10</td>\n",
       "      <td>10.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>184.0</td>\n",
       "      <td>[17031640800, 17031650301, 17031640100, 170316...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3127318</th>\n",
       "      <td>2017-12-31 22:00:00</td>\n",
       "      <td>1.703198e+10</td>\n",
       "      <td>13.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>174.0</td>\n",
       "      <td>[17031640800, 17031650301, 17031640100, 170316...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3127319</th>\n",
       "      <td>2017-12-31 23:00:00</td>\n",
       "      <td>1.703198e+10</td>\n",
       "      <td>2.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>176.0</td>\n",
       "      <td>[17031640800, 17031650301, 17031640100, 170316...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3127320 rows Ã 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  timestamp    pickup_loc  demand  demand_h-1  demand_h-2  \\\n",
       "0       2017-01-01 00:00:00  1.703101e+10     0.0         NaN         NaN   \n",
       "1       2017-01-01 01:00:00  1.703101e+10     0.0         0.0         NaN   \n",
       "2       2017-01-01 02:00:00  1.703101e+10     0.0         0.0         0.0   \n",
       "3       2017-01-01 03:00:00  1.703101e+10     0.0         0.0         0.0   \n",
       "4       2017-01-01 04:00:00  1.703101e+10     0.0         0.0         0.0   \n",
       "...                     ...           ...     ...         ...         ...   \n",
       "3127315 2017-12-31 19:00:00  1.703198e+10     4.0        15.0        14.0   \n",
       "3127316 2017-12-31 20:00:00  1.703198e+10     9.0         4.0        15.0   \n",
       "3127317 2017-12-31 21:00:00  1.703198e+10    10.0         9.0         4.0   \n",
       "3127318 2017-12-31 22:00:00  1.703198e+10    13.0        10.0         9.0   \n",
       "3127319 2017-12-31 23:00:00  1.703198e+10     2.0        13.0        10.0   \n",
       "\n",
       "         demand_h-24  demand_p-24  demand_bucket_2  demand_bucket_6  \\\n",
       "0                NaN          NaN              0.0              0.0   \n",
       "1                NaN          NaN              0.0              0.0   \n",
       "2                NaN          NaN              0.0              0.0   \n",
       "3                NaN          NaN              0.0              0.0   \n",
       "4                NaN          NaN              0.0              0.0   \n",
       "...              ...          ...              ...              ...   \n",
       "3127315         17.0         17.0             19.0             97.0   \n",
       "3127316         16.0         16.0             13.0             88.0   \n",
       "3127317         20.0         20.0             19.0             76.0   \n",
       "3127318         11.0         11.0             23.0             57.0   \n",
       "3127319         20.0         20.0             15.0             46.0   \n",
       "\n",
       "         demand_bucket_24                                          neighbors  \n",
       "0                     0.0  [17031020100, 17031010400, 17031010501, 170310...  \n",
       "1                     0.0  [17031020100, 17031010400, 17031010501, 170310...  \n",
       "2                     0.0  [17031020100, 17031010400, 17031010501, 170310...  \n",
       "3                     0.0  [17031020100, 17031010400, 17031010501, 170310...  \n",
       "4                     0.0  [17031020100, 17031010400, 17031010501, 170310...  \n",
       "...                   ...                                                ...  \n",
       "3127315             204.0  [17031640800, 17031650301, 17031640100, 170316...  \n",
       "3127316             191.0  [17031640800, 17031650301, 17031640100, 170316...  \n",
       "3127317             184.0  [17031640800, 17031650301, 17031640100, 170316...  \n",
       "3127318             174.0  [17031640800, 17031650301, 17031640100, 170316...  \n",
       "3127319             176.0  [17031640800, 17031650301, 17031640100, 170316...  \n",
       "\n",
       "[3127320 rows x 11 columns]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_neighbor_demand(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/lblunck/DEVELOPMENT/AAA_The_Dudes/05_prediction_neighbor_demand2.ipynb Cell 30\u001b[0m in \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/lblunck/DEVELOPMENT/AAA_The_Dudes/05_prediction_neighbor_demand2.ipynb#Y124sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m mean_demand_h_1\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/lblunck/DEVELOPMENT/AAA_The_Dudes/05_prediction_neighbor_demand2.ipynb#Y124sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m# apply the function to each row\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/lblunck/DEVELOPMENT/AAA_The_Dudes/05_prediction_neighbor_demand2.ipynb#Y124sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m df[\u001b[39m'\u001b[39m\u001b[39mmean_neighbor_demand_h-1\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39;49mapply(get_mean_demand_h_1, df\u001b[39m=\u001b[39;49mdf, axis\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/frame.py:9565\u001b[0m, in \u001b[0;36mDataFrame.apply\u001b[0;34m(self, func, axis, raw, result_type, args, **kwargs)\u001b[0m\n\u001b[1;32m   9554\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapply\u001b[39;00m \u001b[39mimport\u001b[39;00m frame_apply\n\u001b[1;32m   9556\u001b[0m op \u001b[39m=\u001b[39m frame_apply(\n\u001b[1;32m   9557\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   9558\u001b[0m     func\u001b[39m=\u001b[39mfunc,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   9563\u001b[0m     kwargs\u001b[39m=\u001b[39mkwargs,\n\u001b[1;32m   9564\u001b[0m )\n\u001b[0;32m-> 9565\u001b[0m \u001b[39mreturn\u001b[39;00m op\u001b[39m.\u001b[39;49mapply()\u001b[39m.\u001b[39m__finalize__(\u001b[39mself\u001b[39m, method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mapply\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/apply.py:746\u001b[0m, in \u001b[0;36mFrameApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    743\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw:\n\u001b[1;32m    744\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_raw()\n\u001b[0;32m--> 746\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/apply.py:873\u001b[0m, in \u001b[0;36mFrameApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    872\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_standard\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 873\u001b[0m     results, res_index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_series_generator()\n\u001b[1;32m    875\u001b[0m     \u001b[39m# wrap results\u001b[39;00m\n\u001b[1;32m    876\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwrap_results(results, res_index)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/apply.py:889\u001b[0m, in \u001b[0;36mFrameApply.apply_series_generator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    886\u001b[0m \u001b[39mwith\u001b[39;00m option_context(\u001b[39m\"\u001b[39m\u001b[39mmode.chained_assignment\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    887\u001b[0m     \u001b[39mfor\u001b[39;00m i, v \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(series_gen):\n\u001b[1;32m    888\u001b[0m         \u001b[39m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[0;32m--> 889\u001b[0m         results[i] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mf(v)\n\u001b[1;32m    890\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(results[i], ABCSeries):\n\u001b[1;32m    891\u001b[0m             \u001b[39m# If we have a view on v, we need to make a copy because\u001b[39;00m\n\u001b[1;32m    892\u001b[0m             \u001b[39m#  series_generator will swap out the underlying data\u001b[39;00m\n\u001b[1;32m    893\u001b[0m             results[i] \u001b[39m=\u001b[39m results[i]\u001b[39m.\u001b[39mcopy(deep\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/apply.py:139\u001b[0m, in \u001b[0;36mApply.__init__.<locals>.f\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mf\u001b[39m(x):\n\u001b[0;32m--> 139\u001b[0m     \u001b[39mreturn\u001b[39;00m func(x, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[1;32m/home/lblunck/DEVELOPMENT/AAA_The_Dudes/05_prediction_neighbor_demand2.ipynb Cell 30\u001b[0m in \u001b[0;36m8\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/lblunck/DEVELOPMENT/AAA_The_Dudes/05_prediction_neighbor_demand2.ipynb#Y124sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39mprint\u001b[39m(i)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/lblunck/DEVELOPMENT/AAA_The_Dudes/05_prediction_neighbor_demand2.ipynb#Y124sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m neighbor_locs \u001b[39m=\u001b[39m row[\u001b[39m'\u001b[39m\u001b[39mneighbors\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/lblunck/DEVELOPMENT/AAA_The_Dudes/05_prediction_neighbor_demand2.ipynb#Y124sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m mean_demand_h_1 \u001b[39m=\u001b[39m df[df[\u001b[39m'\u001b[39;49m\u001b[39mpickup_loc\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49misin(neighbor_locs)][\u001b[39m'\u001b[39m\u001b[39mdemand_h-1\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mmean()\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/lblunck/DEVELOPMENT/AAA_The_Dudes/05_prediction_neighbor_demand2.ipynb#Y124sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mreturn\u001b[39;00m mean_demand_h_1\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/series.py:5563\u001b[0m, in \u001b[0;36mSeries.isin\u001b[0;34m(self, values)\u001b[0m\n\u001b[1;32m   5490\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39misin\u001b[39m(\u001b[39mself\u001b[39m, values) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Series:\n\u001b[1;32m   5491\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   5492\u001b[0m \u001b[39m    Whether elements in Series are contained in `values`.\u001b[39;00m\n\u001b[1;32m   5493\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5561\u001b[0m \u001b[39m    dtype: bool\u001b[39;00m\n\u001b[1;32m   5562\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 5563\u001b[0m     result \u001b[39m=\u001b[39m algorithms\u001b[39m.\u001b[39;49misin(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_values, values)\n\u001b[1;32m   5564\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_constructor(result, index\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex)\u001b[39m.\u001b[39m__finalize__(\n\u001b[1;32m   5565\u001b[0m         \u001b[39mself\u001b[39m, method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39misin\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   5566\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/algorithms.py:523\u001b[0m, in \u001b[0;36misin\u001b[0;34m(comps, values)\u001b[0m\n\u001b[1;32m    520\u001b[0m     comps_array \u001b[39m=\u001b[39m comps_array\u001b[39m.\u001b[39mastype(common, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    521\u001b[0m     f \u001b[39m=\u001b[39m htable\u001b[39m.\u001b[39mismember\n\u001b[0;32m--> 523\u001b[0m \u001b[39mreturn\u001b[39;00m f(comps_array, values)\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36min1d\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/numpy/lib/arraysetops.py:612\u001b[0m, in \u001b[0;36min1d\u001b[0;34m(ar1, ar2, assume_unique, invert)\u001b[0m\n\u001b[1;32m    610\u001b[0m         mask \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros(\u001b[39mlen\u001b[39m(ar1), dtype\u001b[39m=\u001b[39m\u001b[39mbool\u001b[39m)\n\u001b[1;32m    611\u001b[0m         \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m ar2:\n\u001b[0;32m--> 612\u001b[0m             mask \u001b[39m|\u001b[39m\u001b[39m=\u001b[39m (ar1 \u001b[39m==\u001b[39m a)\n\u001b[1;32m    613\u001b[0m     \u001b[39mreturn\u001b[39;00m mask\n\u001b[1;32m    615\u001b[0m \u001b[39m# Otherwise use sorting\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "i=0\n",
    "def get_mean_demand_h_1(row, df):\n",
    "    global i\n",
    "    i+=1\n",
    "    if i%10000==0:\n",
    "        print(i)\n",
    "    neighbor_locs = row['neighbors']\n",
    "    mean_demand_h_1 = df[df['pickup_loc'].isin(neighbor_locs)]['demand_h-1'].mean()\n",
    "    return mean_demand_h_1\n",
    "\n",
    "# apply the function to each row\n",
    "df['mean_neighbor_demand_h-1'] = df.apply(get_mean_demand_h_1, df=df, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function to load data from CSV and preprocess it\n",
    "# def preprocess_census_data():\n",
    "#     df_census_tracts = pd.read_csv(\"data/chicago_census_tracts.csv\")\n",
    "#     df_census_tracts['geometry'] = df_census_tracts['geometry'].apply(wkt.loads)\n",
    "#     df_census_tracts = df_census_tracts.rename(columns={'geometry': 'geometry'})\n",
    "\n",
    "#     gdf = gpd.GeoDataFrame(df_census_tracts, geometry='geometry')\n",
    "#     gdf = gdf.set_crs(epsg=4326, allow_override=True)\n",
    "#     gdf = gdf.to_crs(epsg=3857)\n",
    "#     return gdf\n",
    "\n",
    "# # Function to get neighbors using H3\n",
    "# def get_h3_neighbors(df):\n",
    "#     pickup_locs = df[\"pickup_loc\"].unique()\n",
    "#     return {loc: h3.k_ring(loc, k=1) for loc in pickup_locs}\n",
    "\n",
    "# # Function to get neighbors from Census data\n",
    "# def get_census_neighbors(gdf, buffer_distance=500):\n",
    "#     gdf_buffered = gdf.copy()\n",
    "#     gdf_buffered['geometry'] = gdf.buffer(buffer_distance)\n",
    "#     gdf['geometry'] = gdf_buffered.geometry  # Replace original geometry with buffered one\n",
    "#     census_tract_neighbors = gpd.sjoin(gdf, gdf, how='left', predicate='intersects')\n",
    "#     census_tract_neighbors = census_tract_neighbors[census_tract_neighbors.index != census_tract_neighbors.index_right]\n",
    "#     return census_tract_neighbors.groupby('GEOID10_left')['GEOID10_right'].apply(list).to_dict()\n",
    "\n",
    "# # Function to get neighbor demand by ID\n",
    "# def get_neighbor_demand_by_ID(df_timestamp, pickup_loc, neighbors_dict):\n",
    "#     neighbors = neighbors_dict[pickup_loc]\n",
    "#     neighbor_demand = df_timestamp[df_timestamp['pickup_loc'].isin(neighbors)]['demand_h-1'].mean()\n",
    "#     return neighbor_demand\n",
    "\n",
    "# i = 0\n",
    "# def get_group_demand(df_group, neighbors_dict):\n",
    "#     i+=1\n",
    "#     if i%10000==0:\n",
    "#         print(i)\n",
    "#     df_group['neighbor_demand_h-1'] = df_group.apply(lambda row: get_neighbor_demand_by_ID(df_group, row['pickup_loc'], neighbors_dict), axis=1)\n",
    "#     return df_group\n",
    "\n",
    "# def get_neighbor_demand(df):\n",
    "#     if h3.h3_is_valid(df[\"pickup_loc\"].iloc[0]):\n",
    "#         neighbors_dict = get_h3_neighbors(df)\n",
    "#     else:\n",
    "#         gdf = preprocess_census_data()\n",
    "#         neighbors_dict = get_census_neighbors(gdf)\n",
    "    \n",
    "#     df_grouped = df.groupby('timestamp')\n",
    "#     df = df_grouped.apply(get_group_demand, neighbors_dict=neighbors_dict)\n",
    "#     return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_neighbor_demand()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_neighbor_demand(df):\n",
    "\n",
    "    if h3.h3_is_valid(df[\"pickup_loc\"].iloc[0]):\n",
    "        pickup_locs = df[\"pickup_loc\"].unique()\n",
    "        neighbors_dict = {}\n",
    "        for loc in pickup_locs:\n",
    "            neighbors_dict[loc] = h3.k_ring(loc, k=1)\n",
    "    else:\n",
    "        df_census_tracts = pd.read_csv(\"data/chicago_census_tracts.csv\")\n",
    "        df_census_tracts['the_geom'] = df_census_tracts['the_geom'].apply(shapely.wkt.loads)\n",
    "        df_census_tracts = df_census_tracts.rename(columns={'the_geom': 'geometry'})\n",
    "        df_census_tracts.head()\n",
    "        df_census_tracts = df_census_tracts[[\"geometry\", \"GEOID10\"]]\n",
    "\n",
    "        gdf = gpd.GeoDataFrame(df_census_tracts, geometry='geometry')\n",
    "        gdf = gdf.set_crs(epsg=4326, allow_override=True)\n",
    "        gdf = gdf.to_crs(epsg=3857)\n",
    "\n",
    "        buffer_distance = 500  # buffer distance in meters (500 m here)\n",
    "        gdf_buffered = gdf.copy()\n",
    "        gdf_buffered['geometry'] = gdf.buffer(buffer_distance)\n",
    "        gdf['geometry'] = gdf_buffered.geometry  # Replace original geometry with buffered one\n",
    "        census_tract_neighbors = gpd.sjoin(gdf, gdf, how='left', predicate='intersects')\n",
    "        \n",
    "        # Remove self-matches (where polygons intersect with themselves)\n",
    "        census_tract_neighbors = census_tract_neighbors[census_tract_neighbors.index != census_tract_neighbors.index_right]\n",
    "        census_tract_neighbors_grouped = census_tract_neighbors.groupby('GEOID10_left')['GEOID10_right'].apply(list)\n",
    "        neighbors_dict = census_tract_neighbors_grouped.to_dict()\n",
    "    \n",
    "    def get_neighbor_demand_by_ID(df_timestamp, pickup_loc):\n",
    "        i +=1\n",
    "        if i%10000==0:\n",
    "            print(i)\n",
    "        neighbors = neighbors_dict[pickup_loc]\n",
    "        neighbor_demand = df_timestamp[df_timestamp['pickup_loc'].isin(neighbors)]['demand_h-1'].mean()\n",
    "        return neighbor_demand\n",
    "    \n",
    "    df['neighbor_demand_h-1'] = df.apply(lambda row: get_neighbor_demand_by_ID(df[df['timestamp'] == row['timestamp']], row['pickup_loc']), axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare dataset helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prepared_data(location_type_hexagon=True, resolution=10):\n",
    "    df = load_dataset_to_pandas(location_type_hexagon=location_type_hexagon, resolution=resolution)\n",
    "    print(\"Step 1: Data loaded\")\n",
    "    \n",
    "    df = resample_to_hourly(df)\n",
    "    print(\"Step 2: Data resampled to hourly intervals\")\n",
    "    \n",
    "    # df = merge_weather(df)\n",
    "    # print(\"Step 3: Weather data merged\")\n",
    "    \n",
    "    # df = get_past_temperature(df)\n",
    "    # print(\"Step 4: Past temperature data processed\")\n",
    "    \n",
    "    # df = get_past_precip(df)\n",
    "    # print(\"Step 5: Past precipitation data processed\")\n",
    "    \n",
    "    # df = get_season(df)\n",
    "    # print(\"Step 6: Seasonal data processed\")\n",
    "    \n",
    "    # df = get_weekend(df)\n",
    "    # print(\"Step 7: Weekend data processed\")\n",
    "    \n",
    "    # df = get_daytime(df)\n",
    "    # print(\"Step 8: Daytime data processed\")\n",
    "    \n",
    "    # df = get_holiday_dates(df)\n",
    "    # print(\"Step 9: Holiday data processed\")\n",
    "\n",
    "    df = get_past_single_hour_demand(df)\n",
    "    print(\"Step 11: Past single hour demand data processed\")\n",
    "\n",
    "    # df = get_past_time_bucket_demand(df)\n",
    "    # print(\"Step 12: Past time bucket demand data processed\")\n",
    "    \n",
    "    # df = get_target_time_bucket_demand(df)\n",
    "    # print(\"Step 13: Target time bucket demand data processed\")\n",
    "    \n",
    "    df = get_neighbor_demand(df)\n",
    "    print(\"Step 14: Neighbor demand data processed\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Data loaded\n",
      "Step 2: Data resampled to hourly intervals\n",
      "Step 11: Past single hour demand data processed\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'i' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/lblunck/DEVELOPMENT/AAA_The_Dudes/05_prediction.ipynb Cell 30\u001b[0m in \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/lblunck/DEVELOPMENT/AAA_The_Dudes/05_prediction.ipynb#Y322sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m df_census \u001b[39m=\u001b[39m get_prepared_data(\u001b[39mFalse\u001b[39;49;00m)\n",
      "\u001b[1;32m/home/lblunck/DEVELOPMENT/AAA_The_Dudes/05_prediction.ipynb Cell 30\u001b[0m in \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/lblunck/DEVELOPMENT/AAA_The_Dudes/05_prediction.ipynb#Y322sdnNjb2RlLXJlbW90ZQ%3D%3D?line=29'>30</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mStep 11: Past single hour demand data processed\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/lblunck/DEVELOPMENT/AAA_The_Dudes/05_prediction.ipynb#Y322sdnNjb2RlLXJlbW90ZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39m# df = get_past_time_bucket_demand(df)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/lblunck/DEVELOPMENT/AAA_The_Dudes/05_prediction.ipynb#Y322sdnNjb2RlLXJlbW90ZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39m# print(\"Step 12: Past time bucket demand data processed\")\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/lblunck/DEVELOPMENT/AAA_The_Dudes/05_prediction.ipynb#Y322sdnNjb2RlLXJlbW90ZQ%3D%3D?line=33'>34</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/lblunck/DEVELOPMENT/AAA_The_Dudes/05_prediction.ipynb#Y322sdnNjb2RlLXJlbW90ZQ%3D%3D?line=34'>35</a>\u001b[0m \u001b[39m# df = get_target_time_bucket_demand(df)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/lblunck/DEVELOPMENT/AAA_The_Dudes/05_prediction.ipynb#Y322sdnNjb2RlLXJlbW90ZQ%3D%3D?line=35'>36</a>\u001b[0m \u001b[39m# print(\"Step 13: Target time bucket demand data processed\")\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/lblunck/DEVELOPMENT/AAA_The_Dudes/05_prediction.ipynb#Y322sdnNjb2RlLXJlbW90ZQ%3D%3D?line=37'>38</a>\u001b[0m df \u001b[39m=\u001b[39m get_neighbor_demand(df)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/lblunck/DEVELOPMENT/AAA_The_Dudes/05_prediction.ipynb#Y322sdnNjb2RlLXJlbW90ZQ%3D%3D?line=38'>39</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mStep 14: Neighbor demand data processed\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/lblunck/DEVELOPMENT/AAA_The_Dudes/05_prediction.ipynb#Y322sdnNjb2RlLXJlbW90ZQ%3D%3D?line=40'>41</a>\u001b[0m \u001b[39mreturn\u001b[39;00m df\n",
      "\u001b[1;32m/home/lblunck/DEVELOPMENT/AAA_The_Dudes/05_prediction.ipynb Cell 30\u001b[0m in \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/lblunck/DEVELOPMENT/AAA_The_Dudes/05_prediction.ipynb#Y322sdnNjb2RlLXJlbW90ZQ%3D%3D?line=32'>33</a>\u001b[0m     neighbor_demand \u001b[39m=\u001b[39m df_timestamp[df_timestamp[\u001b[39m'\u001b[39m\u001b[39mpickup_loc\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39misin(neighbors)][\u001b[39m'\u001b[39m\u001b[39mdemand_h-1\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mmean()\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/lblunck/DEVELOPMENT/AAA_The_Dudes/05_prediction.ipynb#Y322sdnNjb2RlLXJlbW90ZQ%3D%3D?line=33'>34</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m neighbor_demand\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/lblunck/DEVELOPMENT/AAA_The_Dudes/05_prediction.ipynb#Y322sdnNjb2RlLXJlbW90ZQ%3D%3D?line=35'>36</a>\u001b[0m df[\u001b[39m'\u001b[39m\u001b[39mneighbor_demand_h-1\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39;49mapply(\u001b[39mlambda\u001b[39;49;00m row: get_neighbor_demand_by_ID(df[df[\u001b[39m'\u001b[39;49m\u001b[39mtimestamp\u001b[39;49m\u001b[39m'\u001b[39;49m] \u001b[39m==\u001b[39;49m row[\u001b[39m'\u001b[39;49m\u001b[39mtimestamp\u001b[39;49m\u001b[39m'\u001b[39;49m]], row[\u001b[39m'\u001b[39;49m\u001b[39mpickup_loc\u001b[39;49m\u001b[39m'\u001b[39;49m]), axis\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/lblunck/DEVELOPMENT/AAA_The_Dudes/05_prediction.ipynb#Y322sdnNjb2RlLXJlbW90ZQ%3D%3D?line=36'>37</a>\u001b[0m \u001b[39mreturn\u001b[39;00m df\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/frame.py:9565\u001b[0m, in \u001b[0;36mDataFrame.apply\u001b[0;34m(self, func, axis, raw, result_type, args, **kwargs)\u001b[0m\n\u001b[1;32m   9554\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapply\u001b[39;00m \u001b[39mimport\u001b[39;00m frame_apply\n\u001b[1;32m   9556\u001b[0m op \u001b[39m=\u001b[39m frame_apply(\n\u001b[1;32m   9557\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   9558\u001b[0m     func\u001b[39m=\u001b[39mfunc,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   9563\u001b[0m     kwargs\u001b[39m=\u001b[39mkwargs,\n\u001b[1;32m   9564\u001b[0m )\n\u001b[0;32m-> 9565\u001b[0m \u001b[39mreturn\u001b[39;00m op\u001b[39m.\u001b[39;49mapply()\u001b[39m.\u001b[39m__finalize__(\u001b[39mself\u001b[39m, method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mapply\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/apply.py:746\u001b[0m, in \u001b[0;36mFrameApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    743\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw:\n\u001b[1;32m    744\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_raw()\n\u001b[0;32m--> 746\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/apply.py:873\u001b[0m, in \u001b[0;36mFrameApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    872\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_standard\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 873\u001b[0m     results, res_index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_series_generator()\n\u001b[1;32m    875\u001b[0m     \u001b[39m# wrap results\u001b[39;00m\n\u001b[1;32m    876\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwrap_results(results, res_index)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/apply.py:889\u001b[0m, in \u001b[0;36mFrameApply.apply_series_generator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    886\u001b[0m \u001b[39mwith\u001b[39;00m option_context(\u001b[39m\"\u001b[39m\u001b[39mmode.chained_assignment\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    887\u001b[0m     \u001b[39mfor\u001b[39;00m i, v \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(series_gen):\n\u001b[1;32m    888\u001b[0m         \u001b[39m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[0;32m--> 889\u001b[0m         results[i] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mf(v)\n\u001b[1;32m    890\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(results[i], ABCSeries):\n\u001b[1;32m    891\u001b[0m             \u001b[39m# If we have a view on v, we need to make a copy because\u001b[39;00m\n\u001b[1;32m    892\u001b[0m             \u001b[39m#  series_generator will swap out the underlying data\u001b[39;00m\n\u001b[1;32m    893\u001b[0m             results[i] \u001b[39m=\u001b[39m results[i]\u001b[39m.\u001b[39mcopy(deep\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[1;32m/home/lblunck/DEVELOPMENT/AAA_The_Dudes/05_prediction.ipynb Cell 30\u001b[0m in \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/lblunck/DEVELOPMENT/AAA_The_Dudes/05_prediction.ipynb#Y322sdnNjb2RlLXJlbW90ZQ%3D%3D?line=32'>33</a>\u001b[0m     neighbor_demand \u001b[39m=\u001b[39m df_timestamp[df_timestamp[\u001b[39m'\u001b[39m\u001b[39mpickup_loc\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39misin(neighbors)][\u001b[39m'\u001b[39m\u001b[39mdemand_h-1\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mmean()\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/lblunck/DEVELOPMENT/AAA_The_Dudes/05_prediction.ipynb#Y322sdnNjb2RlLXJlbW90ZQ%3D%3D?line=33'>34</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m neighbor_demand\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/lblunck/DEVELOPMENT/AAA_The_Dudes/05_prediction.ipynb#Y322sdnNjb2RlLXJlbW90ZQ%3D%3D?line=35'>36</a>\u001b[0m df[\u001b[39m'\u001b[39m\u001b[39mneighbor_demand_h-1\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m row: get_neighbor_demand_by_ID(df[df[\u001b[39m'\u001b[39;49m\u001b[39mtimestamp\u001b[39;49m\u001b[39m'\u001b[39;49m] \u001b[39m==\u001b[39;49m row[\u001b[39m'\u001b[39;49m\u001b[39mtimestamp\u001b[39;49m\u001b[39m'\u001b[39;49m]], row[\u001b[39m'\u001b[39;49m\u001b[39mpickup_loc\u001b[39;49m\u001b[39m'\u001b[39;49m]), axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/lblunck/DEVELOPMENT/AAA_The_Dudes/05_prediction.ipynb#Y322sdnNjb2RlLXJlbW90ZQ%3D%3D?line=36'>37</a>\u001b[0m \u001b[39mreturn\u001b[39;00m df\n",
      "\u001b[1;32m/home/lblunck/DEVELOPMENT/AAA_The_Dudes/05_prediction.ipynb Cell 30\u001b[0m in \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/lblunck/DEVELOPMENT/AAA_The_Dudes/05_prediction.ipynb#Y322sdnNjb2RlLXJlbW90ZQ%3D%3D?line=29'>30</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_neighbor_demand_by_ID\u001b[39m(df_timestamp, pickup_loc):\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/lblunck/DEVELOPMENT/AAA_The_Dudes/05_prediction.ipynb#Y322sdnNjb2RlLXJlbW90ZQ%3D%3D?line=30'>31</a>\u001b[0m     i \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/lblunck/DEVELOPMENT/AAA_The_Dudes/05_prediction.ipynb#Y322sdnNjb2RlLXJlbW90ZQ%3D%3D?line=31'>32</a>\u001b[0m     neighbors \u001b[39m=\u001b[39m neighbors_dict[pickup_loc]\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/lblunck/DEVELOPMENT/AAA_The_Dudes/05_prediction.ipynb#Y322sdnNjb2RlLXJlbW90ZQ%3D%3D?line=32'>33</a>\u001b[0m     neighbor_demand \u001b[39m=\u001b[39m df_timestamp[df_timestamp[\u001b[39m'\u001b[39m\u001b[39mpickup_loc\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39misin(neighbors)][\u001b[39m'\u001b[39m\u001b[39mdemand_h-1\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mmean()\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'i' referenced before assignment"
     ]
    }
   ],
   "source": [
    "df_census = get_prepared_data(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>pickup_loc</th>\n",
       "      <th>trip_start_hour</th>\n",
       "      <th>trip_start_month</th>\n",
       "      <th>trip_start_day</th>\n",
       "      <th>demand</th>\n",
       "      <th>temp</th>\n",
       "      <th>humidity</th>\n",
       "      <th>wind_speed</th>\n",
       "      <th>wind_gust</th>\n",
       "      <th>...</th>\n",
       "      <th>weekend</th>\n",
       "      <th>afternoon_week</th>\n",
       "      <th>afternoon_weekend</th>\n",
       "      <th>evening_week</th>\n",
       "      <th>evening_weekend</th>\n",
       "      <th>morning_week</th>\n",
       "      <th>morning_weekend</th>\n",
       "      <th>night_week</th>\n",
       "      <th>night_weekend</th>\n",
       "      <th>is_holiday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-01-01 00:00:00</td>\n",
       "      <td>842664dffffffff</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1813</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-01-01 01:00:00</td>\n",
       "      <td>842664dffffffff</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2738</td>\n",
       "      <td>33.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-01-01 02:00:00</td>\n",
       "      <td>842664dffffffff</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2758</td>\n",
       "      <td>25.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-01-01 03:00:00</td>\n",
       "      <td>842664dffffffff</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1889</td>\n",
       "      <td>25.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-01-01 04:00:00</td>\n",
       "      <td>842664dffffffff</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1051</td>\n",
       "      <td>25.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            timestamp       pickup_loc  trip_start_hour  trip_start_month   \n",
       "0 2017-01-01 00:00:00  842664dffffffff                0                 1  \\\n",
       "1 2017-01-01 01:00:00  842664dffffffff                1                 1   \n",
       "2 2017-01-01 02:00:00  842664dffffffff                2                 1   \n",
       "3 2017-01-01 03:00:00  842664dffffffff                3                 1   \n",
       "4 2017-01-01 04:00:00  842664dffffffff                4                 1   \n",
       "\n",
       "   trip_start_day  demand  temp  humidity  wind_speed  wind_gust  ...   \n",
       "0               1    1813   NaN       NaN         NaN        NaN  ...  \\\n",
       "1               1    2738  33.0      70.0         8.0        0.0  ...   \n",
       "2               1    2758  25.0      75.0         3.0        0.0  ...   \n",
       "3               1    1889  25.0      75.0         0.0        0.0  ...   \n",
       "4               1    1051  25.0      78.0         5.0        0.0  ...   \n",
       "\n",
       "   weekend  afternoon_week afternoon_weekend evening_week evening_weekend   \n",
       "0        1               0                 0            0               0  \\\n",
       "1        1               0                 0            0               0   \n",
       "2        1               0                 0            0               0   \n",
       "3        1               0                 0            0               0   \n",
       "4        1               0                 0            0               0   \n",
       "\n",
       "  morning_week morning_weekend  night_week  night_weekend  is_holiday  \n",
       "0            0               0           0              1           0  \n",
       "1            0               0           0              1           0  \n",
       "2            0               0           0              1           0  \n",
       "3            0               0           0              1           0  \n",
       "4            0               0           0              1           0  \n",
       "\n",
       "[5 rows x 37 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_demand = get_prepared_data(resolution=4, location_type_hexagon=True)\n",
    "df_demand.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have different measurement scales:\n",
    "\n",
    "Ordinal (natural order, but no quantifiable difference between values or binary):\n",
    "- season_x \n",
    "- trip_start_hour\n",
    "- trip_start_month\n",
    "- trip_start_day\n",
    "Metric (equidistant scale):\n",
    "- temp\n",
    "- demand\n",
    "- precipitation\n",
    "- humidity\n",
    "- wind_speed\n",
    "- wind_gust\n",
    "- pressure\n",
    "Nominal:\n",
    "- public_holiday\n",
    "- weekend\n",
    "- condition_Clear/Sunny etc.\n",
    "\n",
    "We do 2 different analysis:\n",
    "- Metric <-> Metric (Pearson)\n",
    "- Ordinal & Nominal <-> Metric, Ordinal & Nominal <-> Ordinal & Nominal (Spearman)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = df_census[[\"demand\", \"demand_h-1\", \"demand_h-2\", \"demand_h-24\", \"temp\", \"temp_h-1\", \"temp_h-2\", \"temp_h-3\", 'precip', 'precip_h-1', 'precip_h-2', 'precip_h-3', 'humidity', 'wind_speed', 'wind_gust', 'pressure']]\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(metric.corr(method=\"pearson\"), annot=True, cmap=\"RdYlGn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal_and_nominal = df_census[\n",
    "        [\"season_autumn\", \"season_spring\", \"season_summer\", \"season_winter\", \"daytime_morning_week\",\n",
    "        \"daytime_afternoon_week\", \"daytime_evening_week\", \"daytime_night_week\", \"daytime_morning_weekend\",\n",
    "        \"daytime_afternoon_weekend\", \"daytime_evening_weekend\", \"daytime_night_weekend\", \"public_holiday\",\n",
    "        \"weekend\"]] #\"hour\"\n",
    "plt.figure(figsize=(20, 20))\n",
    "sns.heatmap(pd.concat([ordinal_and_nominal, metric], axis=1).corr(method=\"spearman\"), annot=True, cmap=\"RdYlGn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to prevent leakage\n",
    "def get_data_without_demand_features(location_type_hexagon=True, resolution=10):\n",
    "    df = load_dataset_to_pandas(location_type_hexagon=location_type_hexagon, resolution=resolution)\n",
    "    df = resample_to_hourly(df)\n",
    "    df = merge_weather(df)\n",
    "    df = get_past_temperature(df)\n",
    "    df = get_past_precip(df)\n",
    "    df = get_season(df)\n",
    "    df = get_weekend(df)\n",
    "    df = get_daytime(df)\n",
    "    df = get_holiday_dates(df)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_demand_features():\n",
    "    df = get_neighbor_demand(df)\n",
    "    df = get_past_single_hour_demand(df)\n",
    "    df = get_past_time_bucket_demand(df)\n",
    "    df= get_target_time_bucket_demand(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the dataset\n",
    "We split into 70, 15, 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_test_split(df, train_ratio=0.7, val_ratio=0.15):\n",
    "    def split_data(df, train_ratio, val_ratio):\n",
    "        # First, we'll sort the dataframe just in case it isn't sorted\n",
    "        df = df.dropna()\n",
    "        df = df.sort_values('timestamp')\n",
    "        \n",
    "        # calculate the size of each section\n",
    "        train_size = int(len(df) * train_ratio)\n",
    "        val_size = int(len(df) * val_ratio)\n",
    "        \n",
    "        # split the data\n",
    "        train_df = df.iloc[:train_size]\n",
    "        val_df = df.iloc[train_size:train_size+val_size]\n",
    "        test_df = df.iloc[train_size+val_size:]\n",
    "        \n",
    "        return train_df, val_df, test_df\n",
    "\n",
    "    # Apply the function for each 'pickup_loc' category\n",
    "    train_dfs = []\n",
    "    val_dfs = []\n",
    "    test_dfs = []\n",
    "\n",
    "    for pickup_loc in df['pickup_loc'].unique():\n",
    "        df_loc = df[df['pickup_loc'] == pickup_loc]\n",
    "        train_df_loc, val_df_loc, test_df_loc = split_data(df_loc, train_ratio, val_ratio)\n",
    "        \n",
    "        train_dfs.append(train_df_loc)\n",
    "        val_dfs.append(val_df_loc)\n",
    "        test_dfs.append(test_df_loc)\n",
    "\n",
    "    # Concatenate the dataframes\n",
    "    train_df = pd.concat(train_dfs)\n",
    "    val_df = pd.concat(val_dfs)\n",
    "    test_df = pd.concat(test_dfs)\n",
    "    \n",
    "    return train_df, val_df, test_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target Encoding of Pickup Location (Hexagons/Census Tract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_encoding(train_df, val_df, test_df):\n",
    "\n",
    "    # initialize the encoder\n",
    "    encoder = ce.TargetEncoder(cols=['pickup_loc'])\n",
    "\n",
    "    # fit the encoder using the training data\n",
    "    encoder.fit(train_df['pickup_loc'], train_df['demand'])\n",
    "\n",
    "    # transform the data\n",
    "    train_df['pickup_loc'] = encoder.transform(train_df['pickup_loc'])\n",
    "    val_df['pickup_loc'] = encoder.transform(val_df['pickup_loc'])\n",
    "    test_df['pickup_loc'] = encoder.transform(test_df['pickup_loc'])\n",
    "    return train_df, val_df, test_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KERNELS = ['linear', 'poly', 'sigmoid', 'rbf']\n",
    "HORIZONS = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES = ['pickup_loc', 'trip_start_hour', 'trip_start_month',\n",
    "       'trip_start_day', 'demand_h-1', 'demand_h-2', 'demand_h-24', 'temp',\n",
    "       'humidity', 'wind_speed', 'wind_gust', 'pressure', 'precip',\n",
    "       'condition_Clear/Sunny', 'condition_Cloudy', 'condition_Other',\n",
    "       'condition_Rain/Storms', 'condition_Snow/Winter Conditions', 'temp_h-1',\n",
    "       'temp_h-2', 'temp_h-3', 'precip_h-1', 'precip_h-2', 'precip_h-3',\n",
    "       'season_autumn', 'season_spring', 'season_summer', 'season_winter',\n",
    "       'weekend', 'afternoon_week', 'afternoon_weekend', 'evening_week',\n",
    "       'evening_weekend', 'morning_week', 'morning_weekend', 'night_week',\n",
    "       'night_weekend', 'is_holiday']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df, test_df = train_val_test_split(df_census)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_df.drop('demand', axis=1)\n",
    "y_train = train_df['demand']\n",
    "\n",
    "X_val = val_df.drop('demand', axis=1)\n",
    "y_val = val_df['demand']\n",
    "\n",
    "X_test = test_df.drop('demand', axis=1)\n",
    "y_test = test_df['demand']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the model\n",
    "model = svm.SVR()\n",
    "\n",
    "# train the model\n",
    "model.fit(X_train[FEATURES], y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions on the validation set\n",
    "y_val_pred = model.predict(X_val)\n",
    "\n",
    "# make predictions on the test set\n",
    "y_test_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
